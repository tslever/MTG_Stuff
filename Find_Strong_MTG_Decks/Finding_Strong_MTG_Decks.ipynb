{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Strong MTG Decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created: 01/07/19 by Tom Lever\n",
    "<br>\n",
    "Updated: 04/26/19 by Tom Lever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic the Gathering is a beautiful and complex trading card game, for children and adults alike, based on casting spells and either outlasting or defeating one or more opponents. There is a definite mystical and nostalgic atmosphere to the game, and in fact Magic cards are designed to share mechanics and flavors that reflect stories. Magic the Gathering Arena, an internet-based gameplay environment released in Fall 2018, makes playing Magic accessible and captivating, and does an excellent job performing and animating a vast number of intricately related actions. Given how complex playing Magic is and, by extension, how difficult it is to create strong Magic decks, I wrote software that creates strong decks based on gatherer.wizards.com's cards database, mtgarena.pro's deck database, and my own rules-text categories database. In this paper I present my software process and results. A strong deck list involving \"Forest\" and “Plains” cards generated by my software won 2 out of 3 games in the “Ranked” mode of MTG Arena at \"Gold Tier 3\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the most basic level, a game of Magic the Gathering involves two players. Each player begins the game with twenty life points. Each player shuffles their own library of sixty cards and draws a seven-card hand. Players alternate in taking turns. A player's turn consists of a beginning phase, a pre-combat main phase, a combat phase, a post-combat main phase, and an ending phase. Each phase consists of one or more steps. During a main phase of the active player (i.e., the player whose turn it is), the active player may convert a land card in their hand into a land permanent on the battlefield under their control. During any phase, under certain conditions, a player may tap land permanents for the rest of their turn to create mana that can be used within the present step in a process of casting spells. Some spells cast by the active player, usually cast during a main phase of the active player, result in the creation of creature permanents on the battlefield under the caster's control. During the active player's combat phase, the active player often taps one or more creatures to have them attack the other player. During the active player's beginning phase, the active player usually resets tapped permanents to an untapped state and draws a card. During the ending phase, the game automatically performs some actions.\n",
    "\n",
    "What makes Magic such an engaging game is how players and their permanents interact. On a basic level, players can cast some spells, activate some abilities of permanents, and take some other actions, like blocking attacks, during others' turns. On a more interesting level, players have many opportunities during each other's turns to cast spells and activate abilities to negate attacks, strengthen their own creatures or incapacitate others' creatures, better themselves and harm their opponents, and prepare for future turns. On an advanced level, some permanents even have abilities that are triggered automatically when certain events occur, which then allow for other abilities and spells to be activated, triggered, or cast, often resulting in cascades of abilities and spells. The number of interactions, number of cards that may be included in libraries, and the memorable flavors of Magic cards make for complex and beautiful gameplay that rather well imitates what I believe to be a widely held, romantic image of mage battles.\n",
    "\n",
    "The complexity of Magic the Gathering, while rewarding and captivating, makes learning to play Magic and constructing libraries rather difficult. In this paper, I present software to aid in creating strong decks. My software should be of interest to any regular Magic player who has ever been frustrated by the complexity and open-endedness that I believe is inherent in constructing Magic decks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My software for developing strong MTG decks relies on three main datasets. The first is a database of information on 1,617 cards in the Ixalan, Rivals of Ixalan, Dominaria, Core Set 2019, Guilds of Ravnica, and Ravnica Allegiance card sets that I created using information from https://gatherer.wizards.com/.\n",
    "\n",
    "My second main dataset is a database of card names, win / loss ratios, and card counts associated with 2,271 proven decks (proven by virtue of having win / loss ratios) ensured to have sixty cards. I created this database using information from https://mtgarena.pro/decks/?community/.\n",
    "\n",
    "My third main dataset is my own rules-text categories database, which classifies each card in a condensed version of the cards database into one or more of fifty categories based on the content of that card's rules text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare my cards database for analysis, I created a condensed cards database by deleting rows from my cards database with the same name, except the first row. The clearest application of this is aggregating basic land cards found in multiple card sets into one basic land card in one set. This aggregation was an automatic process. The condensed database has 1,546 cards. I also manually adjusted the mana types of lands from being all \"Colorless\" to being \"Colorless\", associated with one specific mana type, associated with two mana types, or associated with any mana type. The rarities for the five basic land cards is “Land” so that the basic lands appear at the top of the best-cards database when the best-cards database is sorted first by rarity and then by total number of occurrences. The condensed cards database is used first to create a best-cards database.\n",
    "\n",
    "Constructing a name x ratio database required locating information on 4,827 decks, then scraping together information on 2,412 proven decks, then extracting information on 2,271 decks ensured to have sixty cards, then creating a table of mtgarena.pro card ID numbers and card names in those 2,271 decks, then creating a ID / name x location / win ratio database filled in with card counts, then aggregating rows by name in and eliminating the ID column and location row from the ID / name x location / win ratio database. Creating the name x ratio database took a lot of prep work and it was a little difficult to reload this table once saved to a CSV file due to duplicate column headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental research question that I answered using my software was, “What is a strong deck of sixty nonland cards where all the cards:\n",
    "<ol>\n",
    "    <li>Are in my MTG-Arena inventory;\n",
    "    <li>Have mana costs in a certain grouping [e.g., (“C”, “F”, \"P\", \"CF\", \"CP\", \"FP\", “CFP”)];\n",
    "    <li>Each belong to one or more categories in a grouping of categories associated with a high win / loss ratio [e.g., (“create creature token or convert lands into creatures”, \"destroy\", \"draw\", \"put nonland card\", \"flying\")];\n",
    "    <li>Are balanced so that more important categories have higher proportions of cards; and\n",
    "    <li>Have a maximum average converted mana cost less than a certain user-defined maximum [e.g., 3.001]?\"\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id=\"tasks_checklist\"></a>Summary of Methods\n",
    "\n",
    "My software, executed one module at a time, completes the tasks in the below checklist.\n",
    "\n",
    "<ol>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#condensing_cards_database\">Condense cards database</a> by deleting rows with the same name, except the first row. The clearest application of this is aggregating basic land cards found in multiple card sets into one basic land card in one set. My condensed database has 1,546 cards. The condensed cards database will be used first to create a best-cards database. After condensing the cards database, I manually adjusted the mana types of lands from being all \"Colorless\" to being \"Colorless\", associated with one mana type, associated with two mana types, or associated with any mana type.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#creating_a_raw_table_of_deck_URLs_and_any_win_loss_ratios\">Create a raw table of deck URL's and any win ratios</a>. The URL's are to information on 4,827 proven or unproven decks read from https://mtgarena.pro/decks/?community/. A URL may not have a corresponding win ratio. A cleaned table will be created using this raw table.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#creating_a_cleaned_table_of_deck_URLs_and_win_loss_ratios\">Create a cleaned table of URL's and win ratios</a>. The URL's are to information on 2,412 proven decks read from https://mtgarena.pro/decks/?community/ and the win / loss ratios corresponding to those decks. The URL's will be used first to archive information on each deck to its own text file. The win / loss ratios will be used first to create a winnowed table of URL's and win / loss ratios for decks ensured to have sixty cards.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#archiving_information_on_each_proven_deck\">Archive information on each of 2,412 proven decks</a> in its own text file in a folder of files of deck information. A similar archive will be created from this archive for information on each of 2,271 decks ensured to have sixty cards. \n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#archiving_information_on_sixty-card_decks_and_winnowing_cleaned_table_of_URLs_and_ratios\">Archive information on sixty-card decks and create a winnowed cleaned table of URLs and ratios.</a> The archived information will be used first to create a table of card ID numbers and names of cards in each deck ensured to have sixty cards. The winnowed cleaned table of URLs and ratios will be used first as the multirow header of a (card ID / card name) $\\times$ (deck URL / win/loss ratio) database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#creating_a_table_of_card_ID_numbers_and_names\">Create a table of card ID numbers and names.</a> Each (ID number, name) pair will correspond to one card in at least one of the 2,271 decks ensured to have sixty cards. Each (ID number, name) pair in the table will be unique. The ID numbers and names for cards in each deck are found in the BeautifulSoup for that deck. Unfortunately, different ID numbers may have the same card name. I handle this by aggregating rows with the same card name in the (card ID / card name) $\\times$ (deck URL / win/loss ratio) database. This table of ID numbers and names will be used first in the multicolumn index of the (card ID / card name) $\\times$ (deck URL / win/loss ratio) database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#creating_a_card_ID_card_name_x_deck_url_win_loss_ratio_database\">Create a (card ID / card name) $\\times$ (deck URL / win/loss ratio) database.</a> A condensed database without MTGArena.pro ID numbers, without deck-page URL's, and with rows aggregated by name will be created using this database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#Creating_a_name_x_ratio_database\">Create a name $\\times$ ratio database.</a> MTGArena.pro ID numbers will be eliminated, URL's will be eliminated, and rows will be aggregated by name. The condensed database will be used first to create a table of card names, total numbers of occurrences, and average frequencies of cards in 2,271 decks.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#creating_a_table_of_card_names_total_occurrences_and_frequencies\">Create a table of card names, total occurrences, and frequencies.</a> The table will be used first to create a best-cards database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#creating_a_best_cards_database\">Create a best-cards database.</a> A best-cards database with an inventory column will be created using the best-cards database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#creating_a_best_cards_database_with_inventory\">Create a best-cards database with inventory.</a> The inventory column of this database will be filled in from an inventory database by an Excel VBA module. The best-cards database will be sorted first by rarity and second by total number of occurrences in 2,271 decks. This database will be filtered into strong decks.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#find_average_win_ratios_of_decks_with_mana_types_in_specific_groupings\">Find average win ratios of decks with mana types in specific groupings.</a> The groupings and average win ratios will be output to this notebook. To satisfy my desires to enjoy play and to play competitively, I will filter the best-cards database with inventory into a strong deck with mana types corresponding to a flavor that I enjoy playing and corresponding to a grouping with a high average win ratio.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#develop_rules_text_categories_database\">Develop rules text categories database.</a> My rules text categories database classifies every card in the condensed cards database as belonging to one or more of fifty categories based on the content of the card's rules text. A categories of interest database will be created using this database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#develop_categories_of_interest_database\">Develop categories of interest database.</a> My categories of interest database classifies every card in the condensed cards database into one or more of half a dozen to a dozen categories of interest. When I only had half of the rules text categories database filled in, I used the below machine-learning techniques to fill in the second half of the appropriate categories of interest columns. When Wizards of the Coast publishes a new card set, I will be able to use the below machine-learning techniques to guess at how the new cards may be assigned to my fifty categories. Regardless of whether or not I am using my machine-learning program to fill in the categories of interest database, I will create a filled-in categories of interest database using the categories of interest database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#fill_categories_of_interest_database\">Fill in categories database</a> by saving a copy of \"Categories_of_Interest.csv\" as \"Filled_In_Categories_Database.csv\" or by using a machine-learning program. A filtered filled-in categories database will be created using this database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#filter_filled_in_categories_database\">Filter filled in categories database</a> by mana type and availability. This database will be used to find the highest average win ratios for all combinations of categories in the filled-in categories database.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#determine_highest_average_win_loss_ratios_for_combinations_of_categories\">Determine highest average win / loss ratios for combinations of categories.</a> For each possible number of unique cards in a deck that each happen to be in at least one category in a grouping, the win / loss ratios of all decks with that number of unique cards in categories are averaged. The highest average win / loss ratio assigned to a grouping of categories is the highest average win / loss ratio among average win / ratios for different numbers of unique cards in categories.\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#weight_categories_in_a_chosen_grouping\">Weight categories in a chosen grouping.</a>\n",
    "        <br>\n",
    "    <li style=\"margin-bottom:32px;\"><a href=\"#develop_strong_deck_list_with_specific_mana_types_categories_and_max_ave_CMC\">Develop a strong deck list with specific mana types, rules-text categories, and maximum average converted mana cost.</a> The mana types were specified in the program that filtered the filled-in categories database. The rules-text categories were specified in the program that developed the categories of interest database. The maximum average converted mana cost is defined in this program. I start of with an unrealistically high max ave CMC (i.e., 12). I vary the max ave CMC based on whether I feel that gameplay with the base unconstrained deck is too \"slow\" / \"heavy\". I am interested in whether constrained decks out-perform unconstrained decks. This strong deck list will be used to play enjoyable and competitive games.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Walking through Software Solutions to Checklist Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. <a id=\"condensing_cards_database\"></a>Condensing Cards Database\n",
    "\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Magic card may be converted into a land permanent; used to cast instant and sorcery spells; or converted into an artifact, creature, artifact creature, enchantment, or planeswalker permanent. Each Magic cards has a name. Each nonland card has a mana cost, which indicates the number and types of land permanents that must be tapped to use that card. Each cards has a high-quality illustration taking up roughly half the card. Each card has a type, and many cards have subtypes. Each card has a set icon, which is filled with one of four colors to indicate rarity. Each instant or sorcery card has rules text that indicates spell effects. Each permanent card has rules text that indicates triggered abilities, or activated abilities and their costs. Each creature card has a power indicator and a toughness indicator, and each planeswalker card has a loyalty indicator.\n",
    "\n",
    "I wrote the below Python program to create a condensed cards database to information on 1,546 cards in my cards database. Rows with the same card information except card set are aggregated. I would like to note that in my original cards database I initially set all lands to have a mana type of Colorless, given that their mana costs are 0. After condensing the cards database, I manually adjusted all land mana types in the condensed cards database to one of \"C\", \"F\", \"I\", \"M\", \"P\", \"S\", \"FI\", \"FM\", \"FP\", \"FS\", \"IM\", \"IP\", \"IS\", \"MP\", \"MS\", \"PS\", or \"Any\". Please see below screenshot of the condensed cards database.\n",
    "\n",
    "My program requires the path to my initial cards database. My program relies on the pandas Python library for reading, aggregating, and writing. My program outputs the condensed cards database to \"Condensed_Cards_Database.csv\" in the \"Data_With_Ravnica_Allegiance\" subfolder to this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-1--Screenshot--Condensed_Cards_Database.png\">\n",
    "<br>\n",
    "<center><b>Figure 1:</b> Screenshot of \"Condensed_Cards_Database.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Condensing_Cards_Database.py\n",
    "#\n",
    "# Created: 03/21/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# This program condenses the cards database by deleting rows with the same name, except the first row.\n",
    "# I manually assigned mana types for lands.\n",
    "#\n",
    "# Inputs: Cards_Database.csv\n",
    "# Dependencies: pandas\n",
    "# Outputs: Condensed_Cards_Database.csv\n",
    "\n",
    "\n",
    "# Allow use of the pandas.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load cards database from file into a pandas DataFrame.\n",
    "cards_database = pd.read_csv(\"Cards_Database.csv\", header=0, index_col=0)\n",
    "\n",
    "# Drop duplicate rows.\n",
    "condensed_cards_database = cards_database[~cards_database.index.duplicated(keep=\"first\")]\n",
    "\n",
    "# Write the condensed cards database to file.\n",
    "condensed_cards_database.to_csv(\"Condensed_Cards_Database.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <a id=\"creating_a_raw_table_of_deck_URLs_and_any_win_loss_ratios\"></a>Creating a Raw Table of Deck URL's and Any Win / Loss Ratios\n",
    "\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Task, Software, and Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to create a raw table of URL's to information on 4,827 proven or unproven decks read from https://mtgarena.pro/decks/?community/ and any win / loss ratios corresponding to those decks. A cleaned table will be created from this raw table. Please see the below image for what my raw table of URL's and win / loss ratios looks like.\n",
    "\n",
    "My program requires no inputs. My program depends on using the Selenium toolset to automate Google Chrome in scrolling through a webpage of dynamically loaded deck information living at http://mtgarena.pro/decks/?community/. My program depends on using the time Python module to put itself to sleep for six seconds after each scroll to give the webpage sufficient time to load new data and, actually, all previous data as well. My program depends on using the BeautifulSoup Python library to structure HTML representing the webpage. My program relies on the re Python module to find deck ID numbers. My program outputs my raw table of deck URL's and any win / loss ratios to \"URLs_n_Ratios.csv\" in the \"Data_With_Ravnica_Allegiance\" subfolder to this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-2--Screenshot--URLs_and_Win_Loss_Ratios.png\">\n",
    "<br>\n",
    "<center><b>Figure 2:</b> Screenshot of \"URLs_n_Ratios.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating_a_Raw_Table_of_Deck_URLs_and_Any_Win-Loss_Ratios.py\n",
    "#\n",
    "# Created: 01/07/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# This program records the dynamically loaded HTML of MTGArena.pro/decks/?community/,\n",
    "# a webpage presenting introductory information for at least 4,827 MTG decks.\n",
    "# The program then creates a table of deck URL's and win / loss ratios.\n",
    "#\n",
    "# Inputs: None\n",
    "# Dependencies: selenium.webdriver, time, bs4.BeautifulSoup, re\n",
    "# Outputs: URLs_n_Ratios.csv,\n",
    "# which contains a 4,828 x 2 table. One row is the table header.\n",
    "\n",
    "\n",
    "# Allow creation of webdriver class instances.\n",
    "from selenium import webdriver\n",
    "\n",
    "# Allow use of the time.sleep method.\n",
    "import time\n",
    "\n",
    "# Allow creation of BeautifulSoup class instances.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Allow use of the re.compile method.\n",
    "import re\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Extract the dynamically loaded HTML of MTGArena.pro/decks/?community/.\n",
    "########################################################################\n",
    "\n",
    "# Open a Chrome browser.\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the webpage.\n",
    "browser.get(\"https://mtgarena.pro/decks/?community\")\n",
    "\n",
    "# Initialize last_height as the webpage's present scroll height.\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# Set a time for the program to sleep between scrolls to allow for the webpage to load.\n",
    "sleep_time  = 6\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Scroll down to bottom of the body.\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Wait to load page.\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    # Calculate new scroll height.\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # If new scroll height equals last scroll height,\n",
    "    # then the webpage hasn't loaded any more information,\n",
    "    # and it's time to end the scrolling.\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    \n",
    "    # If the webpage loaded information, then set the last height to the new height.\n",
    "    last_height = new_height\n",
    "\n",
    "# Extract the HTML representing the fully loaded webpage.\n",
    "inner_HTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "\n",
    "# Close the browser.\n",
    "browser.close()\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# Write each deck URL and win / loss ratio in the loaded HTML\n",
    "# into a table in a CSV file.\n",
    "#############################################################\n",
    "\n",
    "# Parse the loaded HTML into a BeautifulSoup.\n",
    "soup = BeautifulSoup(inner_HTML, \"html.parser\")\n",
    "\n",
    "# Open a CSV file for writing. \n",
    "f = open(\"./Data_With_Ravnica_Allegiance/URLs_n_Ratios.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# Write table headers to the CSV file.\n",
    "f.write(\"Deck-Page URL\" + \",\" + \"Win / Loss Ratio\" + \"\\n\")\n",
    "\n",
    "# For each division block in the BeautifulSoup\n",
    "# that has an ID containing \"deckxx\" and that\n",
    "# represents a deck...\n",
    "for div in soup.findAll('div', id=re.compile('deckxx')):\n",
    "    \n",
    "    # Find the URL associated with that deck.\n",
    "    deck_url = \"https://mtgarena.pro\" + div.find('a')['href']\n",
    "    \n",
    "    # Find the win ratio associated with that deck.\n",
    "    win_ratio = (div.contents)[5].text\n",
    "    if win_ratio != \"??\":\n",
    "        win_ratio = float(win_ratio[:-1])\n",
    "    \n",
    "    # Write the deck URL and deck win ratio to the CSV file.\n",
    "    f.write(deck_url + \",\" + str(win_ratio) + \"\\n\")\n",
    "\n",
    "# Close the file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Ensuring the Best Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>This section applies to data loaded by the above program when https://mtgarena.pro/decks/?community/ only listed decks containing cards from the Ixalan, Rivals of Ixalan, Dominaria, Core Set 2019, and Guilds of Ravnica card sets. I believe that sometime around 03/01/19, a month or so after the Ravnica Allegiance card set was added to MTG Arena, MTGArena.pro reset its community decks listing.</i>\n",
    "\n",
    "I wrote the below Python program to ensure that I would not gain significantly more URL's and win ratios than 13,854 if I changed the sleep time between scrolls through the http://mtgarena.pro/decks/?community/ webpage from 5 seconds to 6 seconds. My program generates a graph showing a logarithmic relationship between number of URL's recorded (in HTML) and total sleep time (the product of sleep time per scroll and the number of sleeps before the program above didn't sleep long enough). You can see below that there is a relatively minute difference in numbers of decks recorded for total sleep times of 24 minutes, 31 minutes, and 39 minutes.\n",
    "\n",
    "My program requires information on the number of URL's recorded in HTML for each sleep time per scroll. My program depends on using the numpy Python package to organize this information. My program depends on using the matplotlib.pyplot Python interface to graph \"Number of URL's Recorded vs. Total Sleep Time\". My program ouputs this graph in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph_Num_URLs_Recorded_vs_Total_Sleep_Time.py\n",
    "#\n",
    "# Created: 01/??/19 by Tom Lever\n",
    "# Updated: 03/19/19 by Tom Lever\n",
    "#\n",
    "# This program graphs the number of URL's recorded in HTML versus total sleep time.\n",
    "# This graph is useful in determining that having a sleep time per load of 5 seconds\n",
    "# is about the maximum sleep time per load that I want, given that I can extrapolate\n",
    "# that there won't be too much more gain in URL's for larger sleep times per scroll.\n",
    "#\n",
    "# Inputs: Array of numbers of URL's recorded for six runs of the program above,\n",
    "# each with a unique sleep time per scroll.\n",
    "# Dependencies: numpy, matplotlib.pyplot\n",
    "# Outputs: Graph of Number of Decks Recorded vs. Total Sleep Time\n",
    "\n",
    "\n",
    "# Allow creation of ndarrays.\n",
    "import numpy as np\n",
    "\n",
    "# Allow use of matplotlib.pyplot scatter-plotting and labeling methods.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# Assemble arrays for number of decks loaded vs. sleep times to load.\n",
    "#####################################################################\n",
    "\n",
    "# Create an ndarray of the number of URL's recorded for each run.\n",
    "nums_URLs_recorded = np.array([30, 3240, 8130, 13737, 13763, 13854])\n",
    "\n",
    "# Calculate the total sleep time corresponding to each number of URL's recorded.\n",
    "total_sleep_times = np.zeros(nums_URLs_recorded.shape[0])\n",
    "decks_added_per_scroll = 30\n",
    "seconds_per_minute = 60\n",
    "for i in range(0, nums_URLs_recorded.shape[0]):\n",
    "    total_sleep_times[i] = i*(nums_URLs_recorded[i]/decks_added_per_scroll - 1)*(1/seconds_per_minute)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# Graph Number of Decks Loaded vs. Minutes of Sleeping Before Loading Failure.\n",
    "##############################################################################\n",
    "\n",
    "# This magic function is apparently important\n",
    "# in getting the scatter plot to show in Jupyter Notebook.\n",
    "# The ordering of the following three code blocks is important.\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(total_sleep_times, nums_URLs_recorded)\n",
    "plt.xlabel(\"Total Sleep Time (min)\")\n",
    "plt.ylabel(\"Number of URL's Recorded\")\n",
    "plt.title(\"Number of URL's Recorded vs. Total Sleep Time\")\n",
    "\n",
    "# This function is important in getting the scatter plot\n",
    "# to show without scatter-plot object information.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. <a id=\"creating_a_cleaned_table_of_deck_URLs_and_win_loss_ratios\"></a>Creating a Cleaned Table of Deck URL's and Win / Loss Ratios\n",
    "\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to create a cleaned table of URL's to information on 2,412 proven decks read from https://mtgarena.pro/decks/?community/ and the win / loss ratios corresponding to those decks. The URL's will be used first to archive information on each deck to its own text file. The win / loss ratios will be used first in a table of deck URL's and win / loss ratios corresponding to decks ensured to have sixty cards. Please see the below screenshot of the cleaned table of URL's and ratios.\n",
    "\n",
    "My program requires the path of my generated raw table of URL's and win / loss ratios. My program depends on using the pandas Python library to load the raw table and to eliminate all rows with the value of \"??\", instances of which I translate into \"NaN\". My program depends on using the numpy Python package to replace all instances of \"??\" with \"NaN\". My program outputs my cleaned table of deck URL's and win / loss ratios to \"URLs_n_Ratios--Cleaned.csv\" in the \"Data_With_Ravnica_Allegiance\" subfolder to this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-3--Screenshot--URLs_and_Win_Loss_Ratios--Cleaned.png\">\n",
    "<br>\n",
    "<center><b>Figure 3:</b> Screenshot of \"URLs_n_Ratios--Cleaned.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating_a_Cleaned_Table_of_Deck_URLs_and_Win-Loss_Ratios.py\n",
    "#\n",
    "# Created: 01/09/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# This program generates a version of \"URLs_n_Ratios.csv\" with rows with no win ratios removed.\n",
    "#\n",
    "# Inputs: \"URLs_n_Ratios.csv\"\n",
    "# Dependencies: pandas, numpy\n",
    "# Outputs: \"URLs_n_Ratios--Cleaned.csv\",\n",
    "# which contains a 2,413 x 2 table. One row is the table header.\n",
    "\n",
    "\n",
    "# Allows using the read_csv, dropna, and to_csv methods.\n",
    "import pandas as pd\n",
    "\n",
    "# Allows replacing \"??\" with NaN.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Read and manipulate the raw table of deck URL's and win ratios as a dataframe.\n",
    "################################################################################\n",
    "\n",
    "# Reads the raw table of deck URL's and win ratios into a dataframe.\n",
    "deck_URLs_and_win_ratios = pd.read_csv(\"./Data_With_Ravnica_Allegiance/URLs_n_Ratios.csv\", header=0, index_col=0)\n",
    "\n",
    "# Replaces all win ratios of \"??\" with NaN.\n",
    "table_with_quest_marks_replaced = deck_URLs_and_win_ratios.replace(\"??\", np.nan)\n",
    "\n",
    "# Removes rows with values of NaN.\n",
    "table_with_NaNs_dropped = table_with_quest_marks_replaced.dropna()\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# Write the cleaned deck-page URL's and win ratios to file.\n",
    "###########################################################\n",
    "\n",
    "table_with_NaNs_dropped.to_csv(\"./Data_With_Ravnica_Allegiance/URLs_n_Ratios--Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. <a id=\"archiving_information_on_each_proven_deck\"></a>Archiving Information on Each Proven Deck\n",
    "\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to load the webpage for each of the 2,412 proven decks and to download the HTML representing the loaded webpage into a text file corresponding to that deck. A set of 2,271 text files will be created from the set of 2,412 text files. Each text file in the set of 2,271 is ensured to correspond to a sixty card deck and to contain no sideboard information. Please see below screenshot of the folder of deck-page HTML's for each of the 2,412 proven decks.\n",
    "\n",
    "My program requires the path of my cleaned table of URL's and win / loss ratios. My program depends on using the pandas Python library to load the cleaned table. My program depends on using the Selenium toolset to automate Google Chrome in loading webpages of deck information. My program outputs a folder of \"Deck-Page_HTMLs\" in the \"Data_Pre_Ravnica_Allegiance\" subfolder of this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-4--Screenshot--Deck-Page_HTMLs.png\" width=50%></td>\n",
    "<br>\n",
    "<center><b>Figure 4:</b> Screenshot of \"./Data_With_Ravnica_Allegiance/Deck-Page_HTMLs\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Archiving_Information_on_Each_Proven_Deck.py\n",
    "#\n",
    "# Created: 01/??/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# This program reads the series of URL's from \"URLs_n_Ratios--Cleaned.csv\"\n",
    "# and archives into a common folder text files containing the HTMLs\n",
    "# representing the web pages accessed via each URL.\n",
    "#\n",
    "# Inputs: URLs_n_Ratios--Cleaned.csv\n",
    "# Dependencies: pandas, selenium.webdriver\n",
    "# Outputs: 2,412 text files containing HTMLs\n",
    "\n",
    "\n",
    "# Allow use of the pd.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "# Allow creation of a webdriver class instance.\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "# Read the URLs from \"URLs_n_Ratios--Cleaned.csv\" into a pandas series.\n",
    "urls = pd.read_csv(\"./Data_With_Ravnica_Allegiance/URLs_n_Ratios--Cleaned.csv\")[\"Deck-Page URL\"]\n",
    "\n",
    "# Open a Chrome browser.\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# For each MTG deck...\n",
    "for i in range(0, len(urls)):\n",
    "\n",
    "    # Open a text file to store the HTML representing the web page for that deck.\n",
    "    f = open(\"./Data_With_Ravnica_Allegiance/Deck-Page_HTMLs/Deck-Page_HTML--\" + str(i) + \".txt\", \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    # Navigate to a webpage via the URL.\n",
    "    browser.get(urls[i])\n",
    "    \n",
    "    # Extract the loaded HTML of the web page and add it to the file.\n",
    "    f.write(browser.execute_script(\"return document.body.innerHTML\"))\n",
    "    \n",
    "    # Close the file.\n",
    "    f.close()\n",
    "    \n",
    "# Close the browser.\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. <a id=\"archiving_information_on_sixty-card_decks_and_winnowing_cleaned_table_of_URLs_and_ratios\"></a>Archiving Information on Sixty-Card Decks and Winnowing Cleaned Table of URLs and Ratios\n",
    "\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to archive information on each deck ensured to have sixty cards, and to create a version of \"URLs_n_Ratios--Cleaned.csv\" that lists URLs to and win ratios for decks ensured to have sixty cards. The program ensures each deck has exactly sixty cards by finding a \"60 / 60\" string in the deck's HTML. The program additionally ensures that it does not confuse sideboard cards with deck cards by eliminating, for each deck, all text mentioning sideboard and all text thereafter. The archive will be used first to create a list of ID numbers and names for cards in the 2,271 decks. The winnowed table of URLs and ratios will be used first as the multirow header of the (card ID / card number) $\\times$ (deck URL / win/loss ratio) database. Please see below screenshots of the folder of deck-page HTML's for each of the 2,271 decks ensured to have sixty cards, and of \"Winnowed_URLs_n_Ratios.csv\".\n",
    "\n",
    "My program requires the path of my cleaned table of URL's and win / loss ratios. My program depends on using the pandas Python library to load the cleaned table. My program depends on using the re Python module to find all text before sideboard-related text. My program outputs a folder of deck-page HTMLs corresponding to sixty-card decks and \"Winnowed_URLs_n_Ratios.csv\" in the \"Data_With_Ravnica_Allegiance\" subfolder of this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"6-5-1--Screenshot--Winnowed_Deck-Page_HTMLs.png\"></td>\n",
    "        <td><img src=\"6-5-2--Screenshot--Winnowed_URLs_n_Ratios.png\"></td>\n",
    "    </tr>\n",
    "    <tr></tr>\n",
    "    <tr>\n",
    "        <td><center><b>Figure 5.1:</b> Screenshot of \"./Data_With_Ravnica_Allegiance/HTMLs--60--Before_Sideboards\"</center></td>\n",
    "        <td><center><b>Figure 5.2:</b> Screenshot of \"Winnowed_URLs_and_Ratios.csv\"</center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Archiving_Information_on_Sixty-Card_Decks_and_Winnowing_URL_and_Win-Loss_Ratio_Table.py\n",
    "#\n",
    "# Created: 01/16/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# This program reads each Deck-Page_HTML-- file and\n",
    "# creates a version of that file, if that file has a 60-card deck,\n",
    "# with HTML before the sideboard, if a sideboard exists,\n",
    "# or all the HTML, if a sideboard does not exist.\n",
    "#\n",
    "# This program also winnows \"URLs_n_Ratios--Cleaned.csv\" to \"Winnowed_URLs_n_Ratios.csv\",\n",
    "# which contains a table of URLs and ratios corresponding to decks ensured to have sixty cards.\n",
    "#\n",
    "# Inputs: Deck-Page_HTML-- files, URLs_n_Ratios--Cleaned.csv\n",
    "# Dependencies: pandas, re\n",
    "# Outputs: 2,271 text files containing winnowed deck-page HTML's, and \"Winnowed_URLs_n_Ratios.csv\".\n",
    "# Winnowed_URLs_n_Ratios contains a 5,391 x 2 table. One row is the table header.\n",
    "\n",
    "\n",
    "# Allows use of the pandas.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "# Allows use of the re.search method.\n",
    "import re\n",
    "\n",
    "\n",
    "# Read the cleaned URL / win ratio table into a dataframe. \n",
    "deck_urls_and_win_ratios = pd.read_csv(\"./Data_Pre_Ravnica_Allegiance/URLs_n_Ratios--Cleaned.csv\")\n",
    "\n",
    "# Enter the \"Deck-Page URL\" column into a list.\n",
    "deck_urls = deck_urls_and_win_ratios[\"Deck-Page URL\"].tolist()\n",
    "\n",
    "# Enter the \"Win / Loss Ratio\" column into a list.\n",
    "win_ratios = deck_urls_and_win_ratios[\"Win / Loss Ratio\"].tolist()\n",
    "\n",
    "# Create an empty list for recording the deck-page URL's for sixty-card decks.\n",
    "winnowed_deck_urls = []\n",
    "\n",
    "# Create an empty list for recording the win ratios for sixty-card decks.\n",
    "winnowed_win_ratios = []\n",
    "\n",
    "\n",
    "# Start a counter for appending to the end of each Deck-Page_HTML-- file corresponding to a sixty-card deck.\n",
    "count = 0\n",
    "\n",
    "# For each original Deck-Page_HTML-- file...\n",
    "for i in range(0, 2411):\n",
    "    \n",
    "    # Read that original Deck-Page_HTML-- file in.\n",
    "    f = open(\"./Data_With_Ravnica_Allegiance/Deck-Page_HTMLs/Deck-Page_HTML--\" + str(i) + \".txt\", \"r\", encoding=\"utf-8\")\n",
    "    deck_HTML = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    # If the present original Deck-Page_HTML-- file represents a 60-card deck...\n",
    "    if \"60 / 60\" in deck_HTML:\n",
    "        \n",
    "        # Write the represented deck's URL into the list of deck URL's for sixty-card decks.\n",
    "        winnowed_deck_urls.append(deck_urls[i])\n",
    "        \n",
    "        # Write the represented deck's win ratio into the list of win ratios for sixty-card decks.\n",
    "        winnowed_win_ratios.append(win_ratios[i])\n",
    "        \n",
    "        # Open a new text file to contain this file exactly or a cropped version,\n",
    "        # depending on whether or not the represented deck has a sideboard.\n",
    "        f = open(\"./Data_With_Ravnica_Allegiance/HTMLs--60--Before_Sideboards/Deck-Page_HTML--\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\")\n",
    "        \n",
    "        # If the represented deck has a sideboard...\n",
    "        if \"dc_dhead\\\">Sideboard\" in deck_HTML:\n",
    "            \n",
    "            # Crop the original deck file and write it into the new file.\n",
    "            f.write(\"<div\" + re.search(\"<div(.*)dc_dhead\\\">Sideboard\", deck_HTML).group(1))\n",
    "        \n",
    "        # If the represented deck does not have a sideboard...\n",
    "        else:\n",
    "            \n",
    "            # Write the original file representing a 60-card deck into the new file.\n",
    "            f.write(deck_HTML)\n",
    "        \n",
    "        # Close the new file.\n",
    "        f.close()\n",
    "        \n",
    "        # Increase the suffix for winnowed deck files by 1.\n",
    "        count += 1\n",
    "\n",
    "\n",
    "# Write the winnowed cleaned table of URL's and win ratios into a CSV file.\n",
    "f = open(\"/Data_With_Ravnica_Allegiance/Winnowed_URLs_n_Ratios.csv\", \"w\", encoding=\"utf-8\")\n",
    "f.write(\"Deck-Page URL\" + \",\" + \"Win / Loss Ratio\" + \"\\n\")\n",
    "for i in range(0, len(winnowed_deck_urls)):\n",
    "    f.write(winnowed_deck_urls[i] + \",\" + str(winnowed_win_ratios[i]) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. <a id=\"creating_a_table_of_card_ID_numbers_and_names\"></a>Creating a Table of Card ID Numbers and Names\n",
    "\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to create a table of card ID numbers and names for each card in the 2,271 decks ensured to have sixty cards. Each (ID number, name) pair will correspond to one card in at least one of the 2,271 decks ensured to have sixty cards. Each (ID number, name) pair in the table will be unique. The ID numbers and names for cards in each deck are found in the BeautifulSoup for that deck. Unfortunately, different ID numbers may have the same card name. I handle this by aggregating rows with the same card name in the (card ID / card name) $\\times$ (deck URL / win/loss ratio) database. This table of ID numbers and names will be used first in the multicolumn index of the (card ID / card name) $\\times$ (deck URL / win/loss ratio) database. Please see below screenshot of the ID's and names table.\n",
    "\n",
    "My program requires the path to the folder of information on the decks ensured to have sixty cards. My program depends on the BeautifulSoup Python library to structure the HTML information for each deck. My program depends on the re Python module to find card ID numbers within specific strings. My program outputs the table of ID's and names to \"IDs_and_Names.csv\" in the \"Data_With_Ravnica_Allegiance\" subfolder of this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-6--Screenshot--IDs_and_Names.png\" width=\"25%\">\n",
    "<br>\n",
    "<center><b>Figure 6:</b> Screenshot of \"IDs_and_Names.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating_a_Table_of_Card_ID_Nums_and_Names.py\n",
    "#\n",
    "# This program creates a list of unique card-ID-number / card-name element pairs across 2,271 decks.\n",
    "#\n",
    "# Created: 01/??/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Winnowed Deck-Page_HTML-- files\n",
    "# Dependencies: bs4.BeautifulSoup, re\n",
    "# Outputs: IDs_and_Names.csv,\n",
    "# which contains a 1,351 x 2 table. One row is the header.\n",
    "\n",
    "\n",
    "# Allow creation of BeautifulSoup class instances.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Allow use of the re.compile method.\n",
    "import re\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# For each of 2,271 decks,\n",
    "# extract from that deck's HTML card-ID-number / card-name element pairs, and\n",
    "# add those element pairs to a running list of element pairs for all decks.\n",
    "#############################################################################\n",
    "\n",
    "# Initialize an empty list for card-ID-number / card-name element pairs.\n",
    "list_of_element_pairs = []\n",
    "\n",
    "# For each winnowed deck-page HTML...\n",
    "for i in range(0, 2270):\n",
    "    \n",
    "    # Read that HTML into a string.\n",
    "    f = open(\"./Data_With_Ravnica_Allegiance/HTMLs--60--Before_Sideboards/Deck-Page_HTML--\" + str(i) + \".txt\", \"r\", encoding=\"utf-8\")\n",
    "    deck_HTML = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Parse the loaded HTML into a BeautifulSoup.\n",
    "    soup = BeautifulSoup(deck_HTML, \"html.parser\")\n",
    "    \n",
    "    # For each division block representing a card in the HTML for the card's web page...\n",
    "    for div in soup.findAll('div', id=re.compile('indeckxx')):\n",
    "        \n",
    "        # Find the card's ID number.\n",
    "        card_ID_num = int(div[\"id\"].split(\"indeckxx\")[1])\n",
    "        \n",
    "        # Find the card's title.\n",
    "        card = div[\"title\"].replace(\",\", \"|\")\n",
    "        \n",
    "        # Add to the list of card-ID-number / card-name element pairs\n",
    "        # an element pair with the present card's ID number and name.\n",
    "        list_of_element_pairs.append([card_ID_num, card])\n",
    "\n",
    "        \n",
    "#################################################################\n",
    "# Create a list of unique element pairs sorted by card ID number.\n",
    "# Write card numbers and card names to a CSV file.\n",
    "#################################################################\n",
    "\n",
    "# Create a list of unique element pairs.\n",
    "list_of_unique_element_pairs = [list(v) for v in dict(list_of_element_pairs).items()]\n",
    "\n",
    "# Sort the list of unique element pairs by card ID number.\n",
    "sorted_list_of_unique_element_pairs = sorted(list_of_unique_element_pairs, key=lambda pair: pair[0])\n",
    "\n",
    "# Extract unique, sorted card ID numbers from the sorted list of unique element pairs.\n",
    "card_ID_nums = [col[0] for col in sorted_list_of_unique_element_pairs]\n",
    "\n",
    "# Extract unique, sorted card names from the sorted list of unique element pairs.\n",
    "card_names = [col[1] for col in sorted_list_of_unique_element_pairs]\n",
    "\n",
    "# Write the card ID numbers and card names to a CSV file.\n",
    "f = open(\"./Data_With_Ravnica_Allegiance/IDs_and_Names--2270_Decks.csv\", \"w\", encoding=\"utf-8\")\n",
    "f.write(\"Card ID Number\" + \",\" + \"Card Name\" + \"\\n\")\n",
    "for i in range(0, len(card_ID_nums)):\n",
    "    f.write(str(card_ID_nums[i]) + \",\" + card_names[i] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. <a id=\"creating_a_card_ID_card_name_x_deck_url_win_loss_ratio_database\"></a>Creating a (Card ID / Card name) $\\times$ (Deck URL / Win/Loss Ratio) Database\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to create a (card ID / card name) $\\times$ (deck URL / win/loss ratio) database. This database will be used first to create a condensed database without MTGArena.pro ID numbers, without deck-page URLs, and with rows aggregated by name. Please see below screenshot of IDs_Names_x_URLs_Ratios_Database.csv.\n",
    "\n",
    "My program requires the path to the table of deck URLs and win / loss ratios for decks ensured to have sixty cards. My program requires the path to the table of card ID numbers and card names. My program relies on the pandas Python library for reading these tables. My program requires the path to the archive of information on decks ensured to have sixty cards. My program depends on the BeautifulSoup Python library for structuring the deck information. My program relies on the re Python module to find the number of instances of each card in each deck. My program relies on the numpy Python package to store the card numbers. My program outputs the (card ID number / card name) $\\times$ (deck URL / win/loss ratio) database to IDs_Names_x_URLs_Ratios_Database.csv in the \"Data_With_Ravnica_Allegiance\" subfolder of this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-7--Screenshot--ID_Name_x_URL_Ratio_Database.png\">\n",
    "<br>\n",
    "<center><b>Figure 7:</b> Screenshot of \"IDs_Nums_x_URLs_Ratios_Database.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating_a_Card_ID_Card_Name_x_Deck_URL_Win-Loss_Ratio_Database.py\n",
    "#\n",
    "# Write to file a (card ID / card name) x (deck URL / win-loss ratio) database\n",
    "# with the number of copies of each card in each deck filling in the table.\n",
    "#\n",
    "# Created: 01/??/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Winnowed_URLs_n_Ratios.csv, IDs_and_Names.csv, winnowed Deck-Page_HTML-- files.\n",
    "# Dependencies: pandas, BeautifulSoup, re, numpy\n",
    "# Outputs: IDs_Names_x_URLs_Ratios_Database.csv\n",
    "\n",
    "\n",
    "# Allow use of the pd.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "# Allow creation of a BeautifulSoup class instance.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Allow use the of the re.compile method.\n",
    "import re\n",
    "\n",
    "# Allow creation of a numpy matrix.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import Winnowed_URLs_n_Ratios.csv.\n",
    "deck_urls_and_win_ratios = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Winnowed_URLs_and_Ratios.csv\")\n",
    "\n",
    "# Send the \"Deck-Page URL\" column to a list.\n",
    "deck_urls = deck_urls_and_win_ratios[\"Deck-Page URL\"].tolist()\n",
    "\n",
    "# Send the \"Win / Loss Ratio\" column to a list.\n",
    "win_ratios = deck_urls_and_win_ratios[\"Win / Loss Ratio\"].tolist()\n",
    "\n",
    "# Import IDs_and_Names--5930_Decks.csv.\n",
    "card_ID_nums_and_names = pd.read_csv(\"./Data_With_Ravnica_Allegiance/IDs_and_Names.csv\")\n",
    "\n",
    "# Send the \"Card ID Number\" column to a list.\n",
    "card_ID_nums = card_ID_nums_and_names[\"Card ID Number\"].tolist()\n",
    "\n",
    "# Send the \"Card Name\" column to a list.\n",
    "card_names = card_ID_nums_and_names[\"Card Name\"].tolist()\n",
    "\n",
    "# Create a card_ID_nums x deck_urls matrix of to store the number of each card in each deck.\n",
    "matrix_of_card_nums = np.zeros((len(card_ID_nums), len(deck_urls))) \n",
    "\n",
    "\n",
    "# For each winnowed deck...\n",
    "for i in range(0, len(deck_urls)):\n",
    "    \n",
    "    # Create a BeautifulSoup of the HTML for the present deck.\n",
    "    f = open(\"./Data_With_Ravnica_Allegiance/HTMLs--60--Before_Sideboards/Deck-Page_HTML--\" + str(i) + \".txt\", \"r\", encoding=\"utf-8\")\n",
    "    deck_HTML = f.read()\n",
    "    soup = BeautifulSoup(deck_HTML, \"html.parser\")\n",
    "    \n",
    "    # For each division block in the Soup representing a card...\n",
    "    for div in soup.findAll('div', id=re.compile('indeckxx')):\n",
    "        \n",
    "        # Find that card's ID number.\n",
    "        card_ID_num = int(div[\"id\"].split(\"indeckxx\")[1])\n",
    "        \n",
    "        # Find the number of copies of the present card in the deck.\n",
    "        num_cards = int(div.find('div', {\"class\": \"dc_ccopies dc_ccc dc_ib\"}).text)\n",
    "        \n",
    "        # Add the number of copies of each card to the appropriate cell in the matrix of card numbers.\n",
    "        matrix_of_card_nums[card_ID_nums.index(card_ID_num), i] = num_cards\n",
    "\n",
    "\n",
    "# Write IDs_Names_x_URLs_Ratios_Database.csv.\n",
    "f = open(\"./Data_With_Ravnica_Allegiance/IDs_Names_x_URLs_Ratios_Database.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "f.write(\",,\")\n",
    "for i in range(0, len(deck_urls)-1):\n",
    "    f.write(deck_urls[i] + \",\")\n",
    "f.write(deck_urls[len(deck_urls)-1] + \"\\n\")\n",
    "\n",
    "f.write(\",,\")\n",
    "for i in range(0, len(deck_urls)-1):\n",
    "    f.write(str(win_ratios[i]) + \",\")\n",
    "f.write(str(win_ratios[len(deck_urls)-1]) + \"\\n\")\n",
    "\n",
    "for i in range(0, len(card_ID_nums)):\n",
    "    f.write(str(card_ID_nums[i]) + \",\" + card_names[i] + \",\")\n",
    "    for j in range(0, len(deck_urls)-1):\n",
    "        f.write(str(matrix_of_card_nums[i, j]) + \",\")\n",
    "    f.write(str(matrix_of_card_nums[i, len(deck_urls)-1]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. <a id=\"Creating_a_name_x_ratio_database\"></a>Creating a Name $\\times$ Ratio Database\n",
    "\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to condense the (card ID / card name) $\\times$ (deck URL / win/loss ratio) database by eliminating the ID column, the URL row, and aggregating rows by name. This database will be used first to create a table of card names and total numbers of occurrences in 2,271 decks. Please see below screenshot of Name_x_Ratio_Database.csv.\n",
    "\n",
    "My program requires the path to the (card ID number / card name) $\\times$ (deck URL / win/loss ratio) database. My program relies on the numpy Python package for storing card numbers in a matrix. My program outputs the condensed database to Name_x_Ratio_Database.csv in the \"Data_With_Ravnica_Allegiance\" subfolder of this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-8--Screenshot--Name_x_Ratio_Database.png\">\n",
    "<br>\n",
    "<center><b>Figure 8:</b> Screenshot of \"Name_x_Ratio_Database.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Condensing_ID_Name_x_URL_Ratio_Database.py\n",
    "#\n",
    "# This program condenses the (card ID / card name) x (deck URL / win-loss ratio) database\n",
    "# by eliminating the card ID column, eliminating the URL row, and aggregating rows by name.\n",
    "#\n",
    "# Created: 03/??/19 by Tom Lever\n",
    "# Updated: 04/07/19 by Tom Lever\n",
    "#\n",
    "# Inputs: IDs_Names_x_URLs_Ratios_Database.csv\n",
    "# Dependencies: numpy\n",
    "# Outputs: Name_x_Ratio_Database.csv\n",
    "\n",
    "\n",
    "# Allow creation of a numpy matrix.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Read from file the ID / name x URL / ratio database into a string.\n",
    "f = open(\"./Data_With_Ravnica_Allegiance/IDs_Names_x_URLs_Ratios_Database.csv\", \"r\", encoding=\"utf-8\")\n",
    "database = f.read()\n",
    "f.close()\n",
    "\n",
    "# Send the rows in the database into a list.\n",
    "rows = list(filter(None, database.split(\"\\n\")))\n",
    "\n",
    "# Send the row of ratios into a list.\n",
    "win_ratios = list(filter(None, rows[1].split(\",\")))\n",
    "\n",
    "# Send the column of names into a list.\n",
    "names = []\n",
    "for i in range(2, len(rows)):\n",
    "    present_row = list(filter(None, rows[i].split(\",\")))\n",
    "    names.append(present_row[1])\n",
    "    \n",
    "for i in range(0, len(names)):\n",
    "    names[i] = names[i].replace(\"Assure // Assemble\", \"Assure // Assemble (Assemble)\")\n",
    "    names[i] = names[i].replace(\"Connive // Concoct\", \"Connive // Concoct (Concoct)\")\n",
    "    names[i] = names[i].replace(\"Discovery // Dispersal\", \"Discovery // Dispersal (Dispersal)\")\n",
    "    names[i] = names[i].replace(\"Expansion // Explosion\", \"Expansion // Explosion (Explosion)\")\n",
    "    names[i] = names[i].replace(\"Find // Finality\", \"Find // Finality (Finality)\")\n",
    "    names[i] = names[i].replace(\"Flower // Flourish\", \"Flower // Flourish (Flourish)\")\n",
    "    names[i] = names[i].replace(\"Integrity // Intervention\", \"Integrity // Intervention (Intervention)\")\n",
    "    names[i] = names[i].replace(\"Invert // Invent\", \"Invert // Invent (Invent)\")\n",
    "    names[i] = names[i].replace(\"Response // Resurgence\", \"Response // Resurgence (Resurgence)\")\n",
    "    names[i] = names[i].replace(\"Status // Statue\", \"Status // Statue (Statue)\")\n",
    "    names[i] = names[i].replace(\"Bedeck // Bedazzle\", \"Bedeck // Bedazzle (Bedazzle)\")\n",
    "    names[i] = names[i].replace(\"Carnival // Carnage\", \"Carnival // Carnage (Carnage)\")\n",
    "    names[i] = names[i].replace(\"Collision // Colossus\", \"Collision // Colossus (Colossus)\")\n",
    "    names[i] = names[i].replace(\"Consecrate // Consume\", \"Consecrate // Consume (Consume)\")\n",
    "    names[i] = names[i].replace(\"Depose // Deploy\", \"Depose // Deploy (Deploy)\")\n",
    "    names[i] = names[i].replace(\"Incubation // Incongruity\", \"Incubation // Incongruity (Incongruity)\")\n",
    "    names[i] = names[i].replace(\"Repudiate // Replicate\", \"Repudiate // Replicate (Replicate)\")\n",
    "    names[i] = names[i].replace(\"Revival // Revenge\", \"Revival // Revenge (Revenge)\")\n",
    "    names[i] = names[i].replace(\"Thrash // Threat\", \"Thrash // Threat (Threat)\")\n",
    "    names[i] = names[i].replace(\"Warrant // Warden\", \"Warrant // Warden (Warden)\")\n",
    "\n",
    "# Send the card-number information that forms the body of the database into a matrix.\n",
    "matrix_of_card_nums = np.zeros((len(names), len(win_ratios)))\n",
    "for i in range(2, len(rows)):\n",
    "    present_row = list(filter(None, rows[i].split(\",\")))\n",
    "    for j in range(2, len(present_row)):\n",
    "        matrix_of_card_nums[i-2, j-2] = present_row[j]\n",
    "        \n",
    "# Aggregate names and matrix of card numbers by name.\n",
    "# Card numbers aggregated with other card numbers are added together.\n",
    "names_aggregated = names\n",
    "matrix_as_list = matrix_of_card_nums.tolist()\n",
    "i = 0\n",
    "while i != len(names_aggregated):\n",
    "    for j in range(0, i):\n",
    "        if names_aggregated[i] == names_aggregated[j]:\n",
    "            for k in range(0, len(matrix_as_list[1])):\n",
    "                matrix_as_list[j][k] += matrix_as_list[i][k]\n",
    "            del names_aggregated[i]\n",
    "            del matrix_as_list[i]\n",
    "            i -= 1\n",
    "            break\n",
    "    i += 1\n",
    "    \n",
    "# Write to file the database with rows aggregated by name.\n",
    "f = open(\"./Data_With_Ravnica_Allegiance/Name_x_Ratio_Database.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "f.write(\",\")\n",
    "for i in range(0, len(win_ratios)-1):\n",
    "    f.write(str(win_ratios[i]) + \",\")\n",
    "f.write(str(win_ratios[len(win_ratios)-1]) + \"\\n\")\n",
    "\n",
    "for i in range(0, len(names_aggregated)):\n",
    "    f.write(names_aggregated[i] + \",\")\n",
    "    for j in range(0, len(win_ratios)-1):\n",
    "        f.write(str(matrix_as_list[i][j]) + \",\")\n",
    "    f.write(str(matrix_as_list[i][len(win_ratios)-1]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. <a id=\"creating_a_table_of_card_names_total_occurrences_and_frequencies\"></a>Creating a Table of Card Names, Total Occurrences, and Frequencies\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to create a table of card names, total numbers of occurrences, and average frequencies in 2,271 decks. This database will be used first to create my list of best cards. Please see below screenshot of Names_Total_Occurrences_and_Freqs.csv.\n",
    "\n",
    "My program requires the path to the name $\\times$ ratio database. My program relies on the numpy Python package for storing card numbers in a matrix. My program outputs the table to Names_Total_Occurrences_and_Freqs.csv in the \"Data_With_Ravnica_Allegiance\" subfolder of this notebook's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-9--Screenshot--Names_Total_Occurrences_and_Freqs.png\" width=\"37.5%\">\n",
    "<br>\n",
    "<center><b>Figure 9:</b> Screenshot of \"Names_Total_Occurrences_and_Freqs.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating_a_Table_of_Card_Names_Total_Occurrences_and_Frequencies.py\n",
    "#\n",
    "# This program creates a table of card names, total number of occurrences, and average frequencies in 2,271 decks.\n",
    "#\n",
    "# Created: 03/??/19 by Tom Lever\n",
    "# Updated: 04/05/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Name_x_Ratio_Database.csv\n",
    "# Dependencies: numpy\n",
    "# Outputs: Names_Total_Occurrences_and_Freqs.csv\n",
    "\n",
    "\n",
    "# Allow creation of a numpy matrix.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Read from file the condensed database into a string.\n",
    "f = open(\"./Data_With_Ravnica_Allegiance/Name_x_URL_Ratio_Database.csv\", \"r\", encoding=\"utf-8\")\n",
    "condensed_database = f.read()\n",
    "f.close()\n",
    "\n",
    "# Send the rows in the condensed database into a list.\n",
    "rows = list(filter(None, condensed_database.split(\"\\n\")))\n",
    "\n",
    "# Send the row of win / loss ratios into a list.\n",
    "ratios = list(filter(None, rows[0].split(\",\")))[1:]\n",
    "\n",
    "# Send the column of card names into a list.\n",
    "names = []\n",
    "for i in range(2, len(rows)):\n",
    "    present_row = list(filter(None, rows[i].split(\",\")))\n",
    "    names.append(present_row[0])\n",
    "\n",
    "# Send the card-number information that forms the body of the database into a matrix.\n",
    "matrix_of_card_nums = np.zeros((len(names), len(ratios)))\n",
    "for i in range(2, len(rows)):\n",
    "    present_row = list(filter(None, rows[i].split(\",\")))\n",
    "    for j in range(2, len(present_row)):\n",
    "        matrix_of_card_nums[i-2, j-2] = present_row[j]\n",
    "        \n",
    "# Create an array of the total number of occurrences of each card.\n",
    "total_occurrences = np.zeros(len(names))\n",
    "for i in range(0, len(names)):\n",
    "    for j in range(0, len(ratios)):\n",
    "        if matrix_of_card_nums[i, j] != 0:\n",
    "            total_occurrences[i] += matrix_of_card_nums[i, j]\n",
    "\n",
    "# Create an array of the average frequencies of each card.\n",
    "frequencies = np.zeros(matrix_of_card_nums.shape[0])\n",
    "num_decks_card_in = np.zeros(matrix_of_card_nums.shape[0])\n",
    "for i in range(0, matrix_of_card_nums.shape[0]):\n",
    "    for j in range(0, matrix_of_card_nums.shape[1]):\n",
    "        if matrix_of_card_nums[i, j] > 0:\n",
    "            frequencies[i] += matrix_of_card_nums[i, j]\n",
    "            num_decks_card_in[i] += 1\n",
    "for i in range(0, matrix_of_card_nums.shape[0]):\n",
    "    frequencies[i] /= num_decks_card_in[i]\n",
    "            \n",
    "# Write the table of card names and total numbers of occurrences to file.\n",
    "f = open(\"./Data_With_Ravnica_Allegiance/Names_Total_Occurrences_and_Freqs.csv\", \"w\", encoding=\"utf-8\")\n",
    "f.write(\"Name\" + \",\" + \"Total Number of Occurrences\" + \",\" + \"Average Frequency\" + \"\\n\")\n",
    "for i in range(0, len(names)):\n",
    "    f.write(names[i] + \",\" + str(total_occurrences[i]) + \",\" + str(frequencies[i]) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. <a id=\"creating_a_best_cards_database\"></a>Creating a Best-Cards Database\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to create a best cards database. A best-cards database with an inventory column will be created using the best-cards database.\n",
    "\n",
    "My program requires the path to the condensed cards database. My program relies on the pandas Python library for loading the condensed cards database. My program requires the path to the name, total occurrences, and frequencies database. My program outputs the best cards database to \"Best_Cards.csv\" in this notebook's \"Data_With_Ravnica_Allegiance\" subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-10--Screenshot--Best_Cards.png\">\n",
    "<br>\n",
    "<center><b>Figure 10:</b> Screenshot of \"Best_Cards.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating_a_Best_Cards_Database.py\n",
    "#\n",
    "# This program creates a best-cards database with columns for\n",
    "# card name, rarity, total occurrences, mana type, average frequency, and rules text.\n",
    "#\n",
    "# Created: 03/??/19 by Tom Lever\n",
    "# Updated: 04/02/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Condensed_Cards_Database.csv, Names_Total_Occurrences_and_Freqs.csv\n",
    "# Dependencies: pandas\n",
    "# Outputs: Best_Cards.csv\n",
    "\n",
    "\n",
    "## Allow use of the pandas.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the condensed cards database into a dataframe.\n",
    "condensed_cards_database = pd.read_csv(\"Condensed_Cards_Database.csv\", index_col=\"Name\", header=0)\n",
    "\n",
    "# Read the table of card names, total occurrences, and average frequencies into a dataframe.\n",
    "names_occurrences_and_freqs = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Names_Total_Occurrences_and_Freqs.csv\", index_col=\"Name\", header=0)\n",
    "\n",
    "# Create a column-wise excerpt of the condensed cards database.\n",
    "excerpt = condensed_cards_database[[\"Rarity\", \"Mana Type\", \"Rules Text\"]]\n",
    "\n",
    "# Merge the condensed cards database and table of names, occurrences, and frequencies into the best cards database.\n",
    "best_cards = names_occurrences_and_freqs.merge(excerpt, left_index=True, right_index=True)\n",
    "\n",
    "# Reorganize the best cards database.\n",
    "best_cards = best_cards[[\"Rarity\", \"Total Number of Occurrences\", \"Mana Type\", \"Average Frequency\", \"Rules Text\"]]\n",
    "\n",
    "# Sort the cards in the best cards database first by rarity and second by occurrences.\n",
    "best_cards = best_cards.sort_values(by=[\"Rarity\", \"Total Number of Occurrences\"], ascending=False)\n",
    "\n",
    "# Write the best cards database to a CSV file.\n",
    "best_cards.to_csv(\"./Data_With_Ravnica_Allegiance/Best_Cards.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11. <a id=\"creating_a_best_cards_database_with_inventory\"></a>Creating Best-Cards Database with Inventory\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Excel VBA module to create a best cards database with columns for card name, rarity, total number of occurrences, mana type, average frequency, inventory, and rules text. The best-cards database will be sorted first by rarity and second by total number of occurrences in 2,271 decks. The best-cards database with inventory will be filtered into strong decks.\n",
    "\n",
    "My VBA module requires that my best cards database and inventory database be in the same Excel Macro-Enabled Workbook. My VBA module requires the Microsoft Scripting Runtime library to be enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-11-1--Screenshot--Best_Cards_with_Inventory.png\">\n",
    "<br>\n",
    "<center><b>Figure 11.1:</b> Screenshot of \"Best Cards\" in \"Best_Cards--with_Inventory.xlsm\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-11-2--Screenshot--Inventory.png\">\n",
    "<br>\n",
    "<center><b>Figure 11.2:</b> Screenshot of \"Inventory\" in \"Best_Cards--with_Inventory.xlsm\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-11-3-1--Screenshot--Enter_Inv_into_Best_Cards_bas.png\">\n",
    "<img src=\"6-11-3-2--Screenshot--Enter_Inv_into_Best_Cards_bas.png\">\n",
    "<br>\n",
    "<center><b>Figure 11.3:</b> Screenshot of \"Enter_Inv_into_Best_Cards.bas\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12. <a id=\"find_average_win_ratios_of_decks_with_mana_types_in_specific_groupings\"></a>Finding Average Win Ratios of Decks with Mana Types in Specific Groupings\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to find the average win ratios of decks with mana types in specific groupings. For example, I found the average win ratios of decks with mana types in the grouping (\"C\", \"I\", \"M\", \"CI\", \"CM\", \"CIM\") to be 53.1 percent. I can use this information to make an educated guess that if I play a deck with mana types in the above grouping I will win more often than if I play a deck with mana types in any other grouping. I will definitely filter the best-cards database with inventory into a strong deck with mana types corresponding to a flavor that I enjoy playing and corresponding to a grouping with a high average win ratio.\n",
    "\n",
    "My program requires the path to the condensed cards database. My program requires the path to the name x ratio database. My program relies on the pandas Python library to read the condensed cards database and name x ratio database into dataframes and to manipulate the dataframes. My program outputs mana type groupings and corresponding average win ratios to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please see below for average win ratios for decks with mana types in certain groupings.\n",
      "C: nan%\n",
      "('C', 'F', 'CF'): 46.0%\n",
      "('C', 'I', 'CI'): 47.0%\n",
      "('C', 'M', 'CM'): 48.4%\n",
      "('C', 'P', 'CP'): 50.6%\n",
      "('C', 'S', 'CS'): 50.3%\n",
      "('C', 'F', 'I', 'FI', 'CFI'): 50.0%\n",
      "('C', 'F', 'M', 'FM', 'CFM'): 51.6%\n",
      "('C', 'F', 'P', 'FP', 'CFP'): 51.3%\n",
      "('C', 'F', 'S', 'FS', 'CFS'): 49.1%\n",
      "('C', 'I', 'M', 'IM', 'CIM'): 53.1%\n",
      "('C', 'I', 'P', 'IP', 'CIP'): 48.4%\n",
      "('C', 'I', 'S', 'IS', 'CIS'): 50.0%\n",
      "('C', 'M', 'P', 'MP', 'CMP'): 49.8%\n",
      "('C', 'M', 'S', 'MS', 'CMS'): 49.8%\n",
      "('C', 'P', 'S', 'PS', 'CPS'): 49.2%\n"
     ]
    }
   ],
   "source": [
    "# Finding_Average_Win_Ratios_of_Decks_with_Mana_Types_in_Specific_Groupings.py\n",
    "#\n",
    "# This program ouputs mana-type groupings and corresponding average win ratios to this notebook.\n",
    "#\n",
    "# Created: 03/??/19 by Tom Lever\n",
    "# Updated: 04/02/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Condensed_Cards_Database.csv, Name_x_Ratio_Database.csv\n",
    "# Dependencies: pandas\n",
    "# Outputs: A table of mana-type groupings and average win ratios\n",
    "\n",
    "\n",
    "# Allow use of the pd.read_csv method and manipulation of imported dataframes.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the condensed cards database into a dataframe, 1546 x 11, with index column and header row additional.\n",
    "condensed_cards_database = pd.read_csv(\"Condensed_Cards_Database.csv\", index_col=0)\n",
    "\n",
    "# Enter the names column of the condensed cards database into a list, 1546 x 1.\n",
    "names_in_CCD = condensed_cards_database.index.tolist()\n",
    "\n",
    "# Enter the mana-types column of the condensed cards database into a list, 1546 x 1.\n",
    "mana_types = condensed_cards_database[\"Mana Type\"].tolist()\n",
    "\n",
    "# Create a table of names and mana types, 1546 x 1, with index column additional.\n",
    "names_and_mana_types = pd.DataFrame(mana_types, columns=[\"Mana Type\"], index=names_in_CCD)\n",
    "\n",
    "\n",
    "# Enter the name x ratio database into a dataframe, 1232 x 2271, with index column and header row additional.\n",
    "name_x_ratio_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Name_x_Ratio_Database.csv\", header=None, index_col=0)\n",
    "name_x_ratio_database.columns = name_x_ratio_database.iloc[0].rename(\"Name\")\n",
    "del name_x_ratio_database.index.name\n",
    "name_x_ratio_database = name_x_ratio_database.iloc[1:]\n",
    "\n",
    "\n",
    "# Create a name and mana type x win / loss ratio dataframe.\n",
    "# Please note that the resulting dataframe is 1225 x 2,272, with index column and header row additional.\n",
    "# Angelic Reward, Confront the Assault, Inspiring Commander, Spiritual Guardian, Tactical Advantage, Angelic Guardian,\n",
    "# and Rampaging Brontodon are not in Magic the Gathering's Gatherer database, and are only in MTG Arena.\n",
    "# The additional column is of course the mana-type column.\n",
    "name_mana_type_x_ratio_database = name_x_ratio_database.merge(names_and_mana_types, left_index=True, right_index=True)\n",
    "\n",
    "# Enter the mana-type column into a series, 1225 x 1, with index column additional.\n",
    "mana_type_column = name_mana_type_x_ratio_database[\"Mana Type\"]\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "# Create a list of long strings, each containing all of the mana costs of all of the cards in a deck.\n",
    "#####################################################################################################\n",
    "\n",
    "list_of_long_strings = []\n",
    "\n",
    "# For each deck in the name and mana type x win / loss ratio database...\n",
    "for j in range(0, name_mana_type_x_ratio_database.shape[1]-1):\n",
    "    \n",
    "    # Create a series of card names and numbers in the present deck.\n",
    "    card_nums = name_mana_type_x_ratio_database.iloc[:, j]\n",
    "    card_nums = card_nums[card_nums != 0]\n",
    "    \n",
    "    # Add the mana type for each card in the present deck to a long string.\n",
    "    long_string = \"\"\n",
    "    for name in card_nums.index:\n",
    "        long_string += str(mana_type_column[name])\n",
    "    \n",
    "    # Add the long string for the present deck to a list of long strings for all decks.\n",
    "    list_of_long_strings.append(long_string)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Create a list of deck mana types.\n",
    "###################################\n",
    "\n",
    "list_of_deck_mana_types = []\n",
    "\n",
    "# For each long string containing all of the mana costs of all of the cards in a deck...\n",
    "for long_string in list_of_long_strings:\n",
    "    \n",
    "    # Create a string of deck mana types.\n",
    "    deck_mana_types = \"\"\n",
    "    if \"C\" in long_string:\n",
    "        deck_mana_types += \"C\"\n",
    "    if \"F\" in long_string:\n",
    "        deck_mana_types += \"F\"\n",
    "    if \"I\" in long_string:\n",
    "        deck_mana_types += \"I\"\n",
    "    if \"M\" in long_string:\n",
    "        deck_mana_types += \"M\"\n",
    "    if \"P\" in long_string:\n",
    "        deck_mana_types += \"P\"\n",
    "    if \"S\" in long_string:\n",
    "        deck_mana_types += \"S\"\n",
    "    \n",
    "    # Add the string of deck mana types to a list of deck mana types for all decks.\n",
    "    list_of_deck_mana_types.append(deck_mana_types)\n",
    "    \n",
    "# Create a table of deck mana types and win ratios, 2271 x 1, with index column additional.\n",
    "mana_types_and_win_ratios = pd.DataFrame(list_of_deck_mana_types, columns=[\"Mana Type\"], index=name_x_ratio_database.columns.values.tolist())\n",
    "\n",
    "print(\"Please see below for average win ratios for decks with mana types in certain groupings.\")\n",
    "\n",
    "# For a mana-type grouping of (\"C\")...\n",
    "mana_type_groupings = [(\"C\")]\n",
    "for grouping in mana_type_groupings:\n",
    "    \n",
    "    # Find the average win ratio for all decks with mana types in the present mana-type grouping.\n",
    "    ave_win_ratio = np.mean(mana_types_and_win_ratios[mana_types_and_win_ratios[\"Mana Type\"] == grouping[0]].index.values.astype(np.float))\n",
    "\n",
    "    groupings.append(grouping)\n",
    "    average_win_ratios.append(ave_win_ratio)\n",
    "    \n",
    "    # Print the present grouping and the average win ratio.\n",
    "    print(str(grouping) + \": %.1f\" % ave_win_ratio + \"%\")\n",
    "    \n",
    "# For each \"mono-colored\" mana-type grouping...\n",
    "mana_type_groupings = [(\"C\", \"F\", \"CF\"),\n",
    "                       (\"C\", \"I\", \"CI\"),\n",
    "                       (\"C\", \"M\", \"CM\"),\n",
    "                       (\"C\", \"P\", \"CP\"),\n",
    "                       (\"C\", \"S\", \"CS\")]\n",
    "\n",
    "for grouping in mana_type_groupings:\n",
    "    \n",
    "    mask = mana_types_and_win_ratios[\"Mana Type\"] == grouping[0]\n",
    "    for i in range(1, len(grouping)):\n",
    "        mask = mask | (mana_types_and_win_ratios[\"Mana Type\"] == grouping[i])\n",
    "    ave_win_ratio = np.mean(mana_types_and_win_ratios[mask].index.values.astype(np.float))\n",
    "    \n",
    "    groupings.append(grouping)\n",
    "    average_win_ratios.append(ave_win_ratio)\n",
    "    \n",
    "    print(str(grouping) + \": %.1f\" % ave_win_ratio + \"%\")\n",
    "    \n",
    "mana_type_groupings = [(\"C\", \"F\", \"I\", \"FI\", \"CFI\"),\n",
    "                       (\"C\", \"F\", \"M\", \"FM\", \"CFM\"),\n",
    "                       (\"C\", \"F\", \"P\", \"FP\", \"CFP\"),\n",
    "                       (\"C\", \"F\", \"S\", \"FS\", \"CFS\"),\n",
    "                       (\"C\", \"I\", \"M\", \"IM\", \"CIM\"),\n",
    "                       (\"C\", \"I\", \"P\", \"IP\", \"CIP\"),\n",
    "                       (\"C\", \"I\", \"S\", \"IS\", \"CIS\"),\n",
    "                       (\"C\", \"M\", \"P\", \"MP\", \"CMP\"),\n",
    "                       (\"C\", \"M\", \"S\", \"MS\", \"CMS\"),\n",
    "                       (\"C\", \"P\", \"S\", \"PS\", \"CPS\")]\n",
    "\n",
    "for grouping in mana_type_groupings:\n",
    "    \n",
    "    mask = mana_types_and_win_ratios[\"Mana Type\"] == grouping[0]\n",
    "    for i in range(1, len(grouping)):\n",
    "        mask = mask | (mana_types_and_win_ratios[\"Mana Type\"] == grouping[i])\n",
    "    ave_win_ratio = np.mean(mana_types_and_win_ratios[mask].index.values.astype(np.float))\n",
    "    \n",
    "    groupings.append(grouping)\n",
    "    average_win_ratios.append(ave_win_ratio)\n",
    "    \n",
    "    print(str(grouping) + \": %.1f\" % ave_win_ratio + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13. <a id=\"develop_rules_text_categories_database\"></a>Developing a Rules Text Categories Database\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I developed a rules text categories database. My rules text categories database classifies every card in the condensed cards database as belonging to one or more of fifty categories based on the content of the card's rules text. A categories of interest database will be created using this database. Please see below screenshot for what my database looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-13--Screenshot--Rules_Text_Categories_Database.png\">\n",
    "<br>\n",
    "<center><b>Figure 13:</b> Screenshot of \"Rules_Text_Categories.csv\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14. <a id=\"develop_categories_of_interest_database\"></a>Developing Categories of Interest Database\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>\n",
    "\n",
    "I developed a categories of interest database by extracting columns of interest from the rules text categories database and reformatting. My categories of interest database classifies every card in the condensed cards database as belonging, or not, to one or more of the categories of interest, based on the content of the card's rules text. When I only had half of the rules text categories database filled in, I used the below machine-learning techniques to fill in the second half of the appropriate categories of interest columns. When Wizards of the Coast publishes a new card set, I will be able to use the below machine-learning techniques to guess at how the new cards may be assigned to my fifty categories. Regardless of whether or not I am using my machine-learning program to fill in the categories of interest database, I will create a filled-in categories of interest database using the categories of interest database. Please see below screenshot for what my categories of interest database looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-14--Screenshot--Categories_of_Interest_Database.png\" width=50%>\n",
    "<br>\n",
    "<center><b>Figure 14:</b> Screenshot of \"Categories_of_Interest.csv\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15. <a id=\"fill_categories_of_interest_database\"></a>Filling Categories of Interest Database\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.15.1. Task, Software, and Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to use a machine learning model to fill in my categories of interest database. A filtered filled-in categories database will be created using this database. The beginning of machine-generated filled-in categories database of course looks the same as the categories of interest database.\n",
    "\n",
    "My program requires the path to the categories of interest database. My program relies on the pandas Python library to read the categories of interest database into a dataframe. My program relies on sklearn.pipeline.Pipeline, sklearn.feature_extraction.text.CountVectorizer, sklearn.feature_extraction.text.TfidfTransformer, and sklearn.linear_model.SGDClassifier classes to develop the machine-learning model. My program relies on numpy Python package to find an model-prediction accuracy for each category. My program writes a filled-in categories database to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following table presents the accuracy of this program's machine-learning model\n",
      "in predicting whether a card in the categories database belongs to a given category.\n",
      "This model will be used to fill in the majority of the categories database.\n",
      "As you can see, the model got an \"A\" for each category.\n",
      "                                                    Accuracy\n",
      "boost power                                            0.940\n",
      "explore                                                1.000\n",
      " flying                                                0.980\n",
      " create creature token or convert lands into cr...     0.980\n",
      " gain life                                             1.000\n",
      " destroy                                               0.970\n",
      " generate nonland mana                                 0.975\n",
      " draw                                                  0.950\n",
      " put nonland card                                      0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Filling_Categories_of_Interest_Database.py\n",
    "#\n",
    "# This program using a machine-learning model to fill in my categories of interest database.\n",
    "#\n",
    "# Created: 04/02/19 by Tom Lever\n",
    "# Updated: 04/17/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Categories_of_Interest.csv\n",
    "# Dependencies: pandas, numpy, sklearn.pipeline.Pipeline, sklearn.feature_extraction.text.CountVectorizer,\n",
    "# sklearn.feature_extraction.text.TfidfTransformer, sklearn.linear_model.SGDClassifier\n",
    "# Outputs: Filled_In_Categories_Database.csv\n",
    "\n",
    "\n",
    "# Allow use of the pandas.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "# Allow creation of a sklearn.pipeline.Pipeline class instance.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Allow the pipeline's fit method to use the sklearn.feature_extraction.text.CountVectorizer class.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Allow the pipeline's fit method to use the sklearn.feature_extraction.text.TfidfTransformer class.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Allow the pipeline's fit method to use the sklearn.linear_model.SGDClassifier class.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Allow use of the numpy.mean method.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Enter the categories_of_interest database into a dataframe.\n",
    "categories_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Categories_of_Interest.csv\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# Determine the accuracy of a machine-learning model that will fill in the majority of the categories database.\n",
    "###############################################################################################################\n",
    "\n",
    "categories = []\n",
    "accuracies = []\n",
    "\n",
    "# Enter the column of rules texts into a list.\n",
    "list_of_rules_texts = categories_database[\"Rules Text\"].tolist()\n",
    "\n",
    "# Designate some of the rules texts as training rules texts.\n",
    "training_rules_texts = list_of_rules_texts[0:950]\n",
    "\n",
    "# Designate some of the rules texts as test rules texts\n",
    "# for determining the accuracy of the model for each category.\n",
    "test_rules_texts = list_of_rules_texts[950:1150]\n",
    "\n",
    "# For each category in the categories database...\n",
    "for category in categories_database.columns.values.tolist()[1:]:\n",
    "    \n",
    "    # Specify a column of training values corresponding to the training rules texts.\n",
    "    training_values = categories_database[category].tolist()[0:950]\n",
    "\n",
    "    # Develop the machine-learning model.\n",
    "    text_clf_svm = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf-svm\", SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-3, n_iter=5, random_state=42))])\n",
    "    text_clf_svm = text_clf_svm.fit(training_rules_texts, training_values)\n",
    "    \n",
    "    # Predict whether each test rules text belongs to the present category.\n",
    "    predicted_svm_values = text_clf_svm.predict(test_rules_texts)\n",
    "\n",
    "    # Find the actual values for whether each test rules text belongs to the present category.\n",
    "    actual_values = categories_database[category].tolist()[950:1150]\n",
    "    \n",
    "    categories.append(category)\n",
    "    accuracies.append(np.mean(predicted_svm_values == actual_values))\n",
    "\n",
    "print(\"The following table presents the accuracy of this program's machine-learning model\")\n",
    "print(\"in predicting whether a card in the categories database belongs to a given category.\")\n",
    "print(\"This model will be used to fill in the majority of the categories database.\")\n",
    "print('As you can see, the model got an \"A\" for each category.')\n",
    "print(pd.DataFrame(accuracies, columns=[\"Accuracy\"], index=categories))\n",
    "\n",
    "\n",
    "##################################\n",
    "# Fill in the categories database.\n",
    "##################################\n",
    "\n",
    "# Enter the column of card names into a list.\n",
    "names = categories_database.index.tolist()\n",
    "\n",
    "filled_in_categories_database = pd.DataFrame(categories_database[\"Rules Text\"].tolist(), columns=[\"Rules Text\"], index=names)\n",
    "    \n",
    "# Specify the rules texts needing corresponding predicted values.\n",
    "test_rules_texts = list_of_rules_texts[950:]\n",
    "\n",
    "for category in categories_database.columns.values.tolist()[1:]:\n",
    "    \n",
    "    # Re-specify each column of training values corresponding to the training rules texts.\n",
    "    training_values = categories_database[category].tolist()[0:950]\n",
    "    \n",
    "    # Redevelop the model.\n",
    "    text_clf_svm = text_clf_svm.fit(training_rules_texts, training_values)\n",
    "    \n",
    "    # Predict the values required to fill in the column corresponding to the present category.\n",
    "    predicted_svm_values = text_clf_svm.predict(test_rules_texts)\n",
    "    \n",
    "    # Define the full column of values corresponding to the present category.\n",
    "    theoretical_values = training_values + predicted_svm_values.tolist()\n",
    "    \n",
    "    # Refresh the column in the categories database corresponding to the present category.\n",
    "    filled_in_categories_database[category] = theoretical_values\n",
    "    \n",
    "# Print the filled in database.\n",
    "filled_in_categories_database.to_csv(\"./Data_With_Ravnica_Allegiance/Filled_In_Categories_Database.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.15.2. Beginning to Think About Teaching a Computer to Come Up with My Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below program and Grammar.fcfg file to acknowledge some ideas I had regarding teaching a computer to come up with my categories in the rules text categories database. Initially, I experimented with finding core transitive verb phrases by replacing \"[LINE BREAK]\" strings with \" \" characters and removing conjunctional, adverbial, and prepositional phrases. I discovered that it was unwieldy to use the re Python package to search for strings that began with pivotal words and ended with punctuation. I then experimented with using Python's Natural Language Tool Kit to parse rules-text sentences into user-defined grammatical structures and return core transitive verb phrases. I successfully parsed a few dozen rules-text sentences, but soon ran into difficulty handling subordinate clauses and sentences where important categorizing information was imbedded in prepositional phrases. Finally, as depicted in the below program and Grammar.fcfg file, I played with the idea of making sentence parsing easier by including more grammar information in the sentences by translating the sentences into a non-English language with more verb conjugations and prepositional phrases instead of attributive nouns. In practice, doing so turned out for me to be an ambiguous and painstaking process. All in all, I am glad I defined by hand my 50 categories in the rules text categories database, especially because my categories changed as I viewed more rules texts, came up with more categories, and grouped categories together.\n",
    "\n",
    "My program relies on Python's Natural Language Tool Kit to load a grammatic rule set that I wrote and to parse sentences into grammatical structures based on that rule set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sentence[struct='fronted']\n",
      "  (Conjunctive_Phrase[]\n",
      "    (Conjunction[] mientras)\n",
      "    (Sentence[struct='simple', tense='pres_prog']\n",
      "      (Noun[] [NAME])\n",
      "      (Verb_Phrase[tense='pres_prog']\n",
      "        (Verb[] está)\n",
      "        (Participle[] atacando))))\n",
      "  (Sentence[struct='simple', tense='pres']\n",
      "    (Noun[] él)\n",
      "    (Verb_Phrase[tense='pres'] (Verb[] consigue) (Noun[] [BOOST]))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"as_long_as [NAME] is attacking| it gets [BOOST].\"\n",
    "sentence = \"mientras [NAME] está atacando| él consigue [BOOST].\"\n",
    "\n",
    "sentence = sentence.replace(\"|\", \"\")\n",
    "sentence = sentence.replace(\".\", \"\")\n",
    "sentence = sentence.split(\" \")\n",
    "\n",
    "chart_parser = nltk.load_parser(\"Grammar.fcfg\")\n",
    "for tree in chart_parser.parse(sentence):\n",
    "    print(str(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-15--Screenshot--Grammar_FCFG_File.png\" width=\"75%\">\n",
    "<br>\n",
    "<center><b>Figure 15:</b> Screenshot of \"Grammar.fcfg\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.16. <a id=\"filter_filled_in_categories_database\"></a>Filtering Filled-In Categories Database\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to filter my filled-in categories database by mana type and availability. This database will be used to find the highest average win ratios for all combinations of categories in the filled-in categories database. Please see below screenshot of the filtered filled-in categories database.\n",
    "\n",
    "My program requires the path to the condensed cards database. My program requires the path to the best-cards database with inventory. My program of course requires the path to the filled-in categories database. My program relies on the pandas Python library to read the databases into dataframes and to manipulate the dataframes. My program outputs the filtered filled-in categories database to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-16--Screenshot--Filtered_Filled_In_Categories_Database.png\" width=\"50%\">\n",
    "<br>\n",
    "<center><b>Figure 16:</b> Screenshot of Filtered Filled-In Categories Database</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtering_Filled_In_Categories_Database.py\n",
    "#\n",
    "# This program filters my filled-in categories database by mana type and availability.\n",
    "#\n",
    "# Created: 04/02/19 by Tom Lever\n",
    "# Updated: 04/08/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Condensed_Cards_Database.csv, Best_Cards--with_Inventory.csv, Filled_In_Categories_Database.csv\n",
    "# Dependencies: pandas\n",
    "# Outputs: Filtered_Filled_In_Categories_Database.csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Enter the condensed cards database into a dataframe.\n",
    "condensed_cards_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Condensed_Cards_Database.csv\")\n",
    "\n",
    "# Create a table of card names and mana types from the condensed cards database.\n",
    "names_and_mana_types = pd.DataFrame(condensed_cards_database[\"Mana Type\"].tolist(), columns=[\"Mana Type\"], index=condensed_cards_database[\"Name\"].tolist())\n",
    "\n",
    "# Filter the names and mana types table into a white-card names and mana types table.\n",
    "mask = names_and_mana_types[\"Mana Type\"] == \"C\"\n",
    "for mana_type in [\"F\", \"P\", \"CF\", \"CP\", \"FP\", \"CFP\", \"Any\"]:\n",
    "    mask = mask | (names_and_mana_types[\"Mana Type\"] == mana_type)\n",
    "\n",
    "names_and_mana_types_filtered_by_mana_type = names_and_mana_types[mask]\n",
    "\n",
    "\n",
    "# Enter the best cards database with inventory into a dataframe.\n",
    "best_cards_database_with_inventory = pd.read_csv(\"./Data_with_Ravnica_Allegiance/Best_Cards--with_Inventory.csv\", index_col=0).fillna(0)\n",
    "\n",
    "# Create a table of card names and inventories from the best cards with inventory database.\n",
    "names_and_inventories = pd.DataFrame(best_cards_database_with_inventory[\"Inventory\"], index=best_cards_database_with_inventory.index)\n",
    "\n",
    "# Filter the names and inventories table into an available names and inventories table.\n",
    "is_available = names_and_inventories[\"Inventory\"] > 0\n",
    "names_and_inventories_filtered_by_inventory = names_and_inventories[is_available]\n",
    "\n",
    "\n",
    "# Enter the filled in categories database into a dataframe.\n",
    "filled_in_categories_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Filled_In_Categories_Database.csv\", header=0, index_col=0)\n",
    "\n",
    "# Filter the filled in categories database by mana type and availability.\n",
    "filtered_filled_in_categories_database = filled_in_categories_database.merge(names_and_mana_types_filtered_by_mana_type, left_index=True, right_index=True)\n",
    "filtered_filled_in_categories_database = filtered_filled_in_categories_database.merge(names_and_inventories_filtered_by_inventory, left_index=True, right_index=True)\n",
    "\n",
    "# Write the filtered filled-in categories database to a CSV file.\n",
    "filtered_filled_in_categories_database.to_csv(\"./Data_With_Ravnica_Allegiance/Filtered_Filled_In_Categories_Database.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.17. <a id=\"determine_highest_average_win_loss_ratios_for_combinations_of_categories\"></a>Determining Highest Average Win / Loss Ratios for Combinations of Categories\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to determine the highest average win / loss ratio for each grouping of categories in the categories of interest database. For each possible number of unique cards in a deck that each happen to be in at least one category in a grouping, the win / loss ratios of all decks with that number of unique cards are averaged. The highest average win / loss ratio assigned to a grouping of categories is the highest average win / loss ratio among average win / ratios for different numbers of unique cards in categories. Please see below screenshot of my table of category combinations / groupings and corresponding highest average win / loss ratios. \n",
    "\n",
    "My program requires the path to my name x ratio database. My program relies on the pandas Python library to read the name x ratio database into a dataframe. My program relies on the itertools.combinations class to create a list of all combinations of categories in the categories of interest database. My program relies on the numpy Python package to create storage for all average win / loss ratios for a given category grouping. My program outputs a table of category groupings and corresponding highest average win / loss ratios to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-17--Screenshot--Categories_and_Ratios_Database.png\">\n",
    "<br>\n",
    "<center><b>Figure 17:</b> Screenshot of Category_Combinations_and_Ratios.csv</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determining_Highest_Average_Win_Loss_Ratios_for_Combinations_of_Categories.py\n",
    "#\n",
    "# This program creates a table of highest average win / loss ratios\n",
    "# for all combinations of categories in the filtered filled-in categories database.\n",
    "# The table is sorted in descending order by highest average win / loss ratio.\n",
    "#\n",
    "# Created: 04/02/19 by Tom Lever\n",
    "# Updated: 04/08/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Name_x_Ratio_Database.csv\n",
    "# Dependencies: pandas, itertools.combinations, numpy\n",
    "# Outputs: Category_Combinations_and_Ratios.csv\n",
    "\n",
    "\n",
    "# Allow use of the pandas.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "# Allow creation of an itertools.combinations class instance.\n",
    "from itertools import combinations\n",
    "\n",
    "# Allow creation of a numpy ndarray of zeros.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "list_of_groupings = []\n",
    "list_of_highest_average_ratios = []\n",
    "\n",
    "\n",
    "# Enter the filtered filled in categories database into a dataframe.\n",
    "filtered_filled_in_categories_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Filtered_Filled_In_Categories_Database.csv\", index_col=0, header=0)\n",
    "\n",
    "# Enter the name x ratio database into a dataframe, 1232 x 2271, with index column and header row additional.\n",
    "name_x_ratio_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Name_x_Ratio_Database.csv\", header=None, index_col=0)\n",
    "name_x_ratio_database.columns = name_x_ratio_database.iloc[0].rename(\"Name\")\n",
    "del name_x_ratio_database.index.name\n",
    "name_x_ratio_database = name_x_ratio_database.iloc[1:]\n",
    "\n",
    "\n",
    "# For each possible number of categories in a group...\n",
    "for i in range(1, len(filtered_filled_in_categories_database.columns[1:-2]) + 1):\n",
    "\n",
    "    # Create a list of all possible groupings of categories.\n",
    "    list_of_groupings_of_categories = list(combinations(filtered_filled_in_categories_database.columns[1:-2], i))\n",
    "    \n",
    "    # For each grouping of categories...\n",
    "    for grouping_of_categories in list_of_groupings_of_categories:\n",
    "\n",
    "        # For each category, add the corresponding column from the filtered, filled-in categories database\n",
    "        # to a name x category database.\n",
    "        name_x_category_database = pd.DataFrame(filtered_filled_in_categories_database[grouping_of_categories[0]])\n",
    "        for j in range(1, len(grouping_of_categories)):\n",
    "            name_x_category_database[grouping_of_categories[j]] = filtered_filled_in_categories_database[grouping_of_categories[j]]\n",
    "        \n",
    "        # Create a name x ratio and category database.\n",
    "        name_x_ratio_and_category_database = name_x_ratio_database.merge(name_x_category_database, left_index=True, right_index=True)\n",
    "\n",
    "        # Filter the name x ratio and category database into a dataframe of cards\n",
    "        # with each card in at least one of the categories in the present grouping.\n",
    "        mask = name_x_ratio_and_category_database[grouping_of_categories[0]] == 1\n",
    "        for j in range(1, len(grouping_of_categories)):\n",
    "            mask = mask | name_x_ratio_and_category_database[grouping_of_categories[j]]\n",
    "        filtered_name_x_ratio_and_category_database = name_x_ratio_and_category_database[mask]\n",
    "        \n",
    "        # For each deck in the name x ratio database...\n",
    "        numbers = []\n",
    "        for j in range(0, name_x_ratio_database.shape[1]-1):\n",
    "            \n",
    "            # For the present deck, find all cards that are in at least one category in the present grouping.\n",
    "            cards = filtered_name_x_ratio_and_category_database.iloc[:, j]\n",
    "            cards = cards[cards != 0]\n",
    "            \n",
    "            # For the present deck, record the total number of unique cards,\n",
    "            # each of which is in at least one category in the present grouping.\n",
    "            numbers.append(cards.shape[0])\n",
    "\n",
    "        # Create a table of numbers of unique cards and win ratios.\n",
    "        ratios = name_x_ratio_database.columns.values.tolist()\n",
    "        numbers_and_ratios = pd.DataFrame(list(map(list, zip(*[numbers, ratios]))), columns=[\"Number\", \"Ratio\"])\n",
    "\n",
    "        # For each possible number of unique cards...\n",
    "        average_win_ratios = np.zeros(60)\n",
    "        for j in range(0, 60):\n",
    "            \n",
    "            # Create a list of win ratios for decks with the present number of unique cards.\n",
    "            list_of_win_ratios_for_decks_with_j_unique_cards = [float(k) for k in numbers_and_ratios[numbers_and_ratios[\"Number\"] == 1][\"Ratio\"].tolist()]\n",
    "            \n",
    "            # If the list is not empty...\n",
    "            if len(list_of_win_ratios_for_decks_with_j_unique_cards) != 0:\n",
    "                \n",
    "                # Find the average win ratio for decks with the present number of unique cards.\n",
    "                average_win_ratio = np.mean(list_of_win_ratios_for_decks_with_j_unique_cards)\n",
    "                \n",
    "                # Add the average win ratio to a list of average win ratios for all possible numbers of unique cards.\n",
    "                average_win_ratios[j] = average_win_ratio\n",
    "        \n",
    "        # Add the present grouping of categories to a list.\n",
    "        list_of_groupings.append(grouping_of_categories)\n",
    "        \n",
    "        # Add the highest of the average win ratios to a list.\n",
    "        list_of_highest_average_ratios.append(average_win_ratios.max())\n",
    "        \n",
    "# Write a table of combinations of categories and corresponding highest average win ratios to a CSV file.\n",
    "# The table will be sorted in descending order by highest average win ratio.\n",
    "combination_and_ratio_database = pd.DataFrame(list_of_highest_average_ratios, columns=[\"Highest Average Ratio\"], index=list_of_groupings)\n",
    "combination_and_ratio_database = combination_and_ratio_database.sort_values(by=[\"Highest Average Ratio\"], ascending=False)\n",
    "combination_and_ratio_database.to_csv(\"./Data_With_Ravnica_Allegiance/Category_Combinations_and_Ratios.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.18. <a id=\"weight_categories_in_a_chosen_grouping\"></a>Weighting Categories in a Chosen Grouping\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to weight categories in grouping that looks promising for enjoyable and competitive gameplay. The initial weighting for a category is based on how much that category seems to contribute to the win ratio associated with the present grouping. I calculate the initial weighting of each category as the difference between the win ratio associated with the present grouping of categories and the win ratio associated with a grouping of categories without the present category. This calculation may be zero or negative, because I might choose a grouping of categories in my categories of interest database to try in MTG Arena that has a theoretical win ratio equal to or slightly less than the theoretical win ratio associated with a grouping that is my chosen grouping less the present category.\n",
    "\n",
    "After acquiring the initial weighting of each category in the present grouping, I scale all the calculated weightings by dividing them by the smallest magnitude among weightings, and round these scaled weightings to the nearest whole number. I adjust all zero and negative weightings to 1, because I am insisting on including cards from categories that seem to dilute the potency of a theoretically more competitive deck. I interpret the scaled weighting for a category as the number of cards of that category that I want in a deck with no maximum number of cards.\n",
    "\n",
    "After acquiring the scaled weighting / importance of each category in the present grouping, I re-scale and re-round all the scaled importances so that there sum is close to 36 (i.e., the number of nonland cards I want in a 60-card MTG deck). Again, if a re-scaled importance of a category is zero, because the importances of the other categories are so much higher, I force the re-scaled importance to 1.\n",
    "\n",
    "My end goal is to have a sum of importances equal to 36, so that I can have a number of nonland cards equal to the importance of each category representing that category in MTG Arena deck. While the sum of importances is not equal to 36, I increase the importance of \"flying\", because I believe that all of my MTG Arena decks at this point need as many good flyers as they can afford, and re-scale and re-round until the sum of importances equals 36.\n",
    "\n",
    "Please see below screenshot of \"Categories_and_Importances.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-18--Screenshot--Categories_and_Importances.png\">\n",
    "<br>\n",
    "<center><b>Figure 18:</b> Screenshot of \"Categories_and_Importances.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Importance\n",
      "boost power                                           0.596798\n",
      "flying                                                0.351913\n",
      "create or replace creature token| or convert in...    1.023457\n",
      "destroy                                               0.948752\n",
      "draw                                                  1.249023\n"
     ]
    }
   ],
   "source": [
    "# Weighting_Categories_in_a_Chosen_Grouping.py\n",
    "#\n",
    "# This program creates a table of categories in a chosen grouping\n",
    "# and the calculated weights or numbers of cards that should\n",
    "# represent each category in a MTG-Arena deck of the chosen grouping.\n",
    "#\n",
    "# Created: 04/02/19 by Tom Lever\n",
    "# Updated: 04/20/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Category_Combinations_and_Ratios.csv\n",
    "# Dependencies: pandas\n",
    "# Outputs: Categories_and_Importances.csv\n",
    "\n",
    "\n",
    "# Allow use of the pandas.read_csv method.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "##############################\n",
    "# Create a list of categories.\n",
    "##############################\n",
    "\n",
    "# Read the table of groupings of categories and associated highest average win ratios into a dataframe.\n",
    "categories_and_ratios = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Category_Combinations_and_Ratios.csv\", index_col=0, header=0)\n",
    "\n",
    "# Select a row corresponding to a grouping to create an table of category weights / importances.\n",
    "desired_row = 7\n",
    "\n",
    "# Convert the string representing the chosen grouping of categories into a list of categories.\n",
    "list_of_categories = categories_and_ratios.index[desired_row][1:-1].split(\", \")\n",
    "list_of_categories = [category[1:-1] if category else None for category in list_of_categories]\n",
    "\n",
    "#############################################################\n",
    "# Create a list of initial category weightings / importances.\n",
    "#############################################################\n",
    "\n",
    "list_of_importances = []\n",
    "\n",
    "# For each category in the above list of categories...\n",
    "for category in list_of_categories:\n",
    "\n",
    "    # Create a string version of the above list of categories, but without the present category.\n",
    "    list_of_categories_wo_present_category = []\n",
    "    for i in range(0, len(list_of_categories)):\n",
    "        if list_of_categories[i] != category:\n",
    "            list_of_categories_wo_present_category.append(list_of_categories[i])\n",
    "    str_of_categories_wo_present_category = str(tuple(list_of_categories_wo_present_category))\n",
    "\n",
    "    for grouping in categories_and_ratios.index:\n",
    "        if grouping == str_of_categories_wo_present_category:\n",
    "            importance = categories_and_ratios.iloc[desired_row][\"Highest Average Ratio\"] - categories_and_ratios.loc[grouping][\"Highest Average Ratio\"]\n",
    "            list_of_importances.append(importance)\n",
    "            \n",
    "####################################################################\n",
    "# Create a table of categories and initial weightings / importances.\n",
    "####################################################################\n",
    "            \n",
    "categories_and_importances = pd.DataFrame(list_of_importances, columns=[\"Importance\"], index=list_of_categories)\n",
    "categories_and_importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(categories_and_importances)\n",
    "\n",
    "###############################################################\n",
    "# Scale the initial weightings / importances\n",
    "# by dividing them by the smallest magnitude among importances,\n",
    "# and rounding to the nearest whole number.\n",
    "#\n",
    "# Force all non-positive importances to 1.\n",
    "###############################################################\n",
    "\n",
    "scaled_cats_and_imps = categories_and_importances / abs(categories_and_importances[\"Importance\"]).min()\n",
    "scaled_cats_and_imps = scaled_cats_and_imps.round(0)\n",
    "\n",
    "forced_importance = 1\n",
    "for category in scaled_cats_and_imps.index:\n",
    "    if scaled_cats_and_imps.loc[category][\"Importance\"] <= 0:\n",
    "        scaled_cats_and_imps.at[category, \"Importance\"] = forced_importance\n",
    "\n",
    "        \n",
    "###################################################################\n",
    "# Re-scale the scaled importances so that there sum is close to 36.\n",
    "#\n",
    "# Force all importances of zero to 1.\n",
    "###################################################################\n",
    "\n",
    "scaled_cats_and_imps = scaled_cats_and_imps / scaled_cats_and_imps[\"Importance\"].sum() * 36\n",
    "scaled_cats_and_imps = scaled_cats_and_imps.round(0)\n",
    "\n",
    "for category in scaled_cats_and_imps.index:\n",
    "    if scaled_cats_and_imps.loc[category][\"Importance\"] == 0:\n",
    "        scaled_cats_and_imps.at[category, \"Importance\"] = forced_importance\n",
    "\n",
    "        \n",
    "#############################################################\n",
    "# While the sum of the scaled importances is greater than 36,\n",
    "# increase importances of 1 by 1, re-scale, and re-round.\n",
    "#############################################################\n",
    "\n",
    "while scaled_cats_and_imps[\"Importance\"].sum() != 36:\n",
    "    if \"flying\" in scaled_cats_and_imps.index.tolist():\n",
    "        scaled_cats_and_imps.loc[\"flying\"] += 1\n",
    "    else:\n",
    "        scaled_cats_and_imps.loc[\"flying\"] = 1    \n",
    "\n",
    "        scaled_cats_and_imps = scaled_cats_and_imps / scaled_cats_and_imps[\"Importance\"].sum() * 36\n",
    "        scaled_cats_and_imps = scaled_cats_and_imps.round(0)\n",
    "\n",
    "    \n",
    "######################################################################\n",
    "# Output the table of categories and scaled importances to a CSV file.\n",
    "######################################################################\n",
    "    \n",
    "scaled_cats_and_imps.to_csv(\"./Data_With_Ravnica_Allegiance/Categories_and_Importances.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.19. <a id=\"develop_strong_deck_list_with_specific_mana_types_categories_and_max_ave_CMC\"></a>Developing Strong Deck List with Specific Mana Types, Weighted Categories, and Maximum Average Converted Mana Cost\n",
    "<br>\n",
    "<a href=\"#tasks_checklist\">Return to Tasks Checklist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the below Python program to develop a strong deck with mana types specified while filtering the filled-in categories database, rules-text categories specified when developing the categories of interest database and weighted according to a table of categories and importances, and a maximum average converted mana cost among nonland cards specified in this program. I start with a unrealistically high max ave CMC (i.e., 12). I vary the max ave CMC based on whether I feel that the base unconstrained deck is too \"slow\" / \"heavy\". This program will be used to play enjoyable and competitive games. Please see below screenshot of \"Strong_Deck.csv\".\n",
    "\n",
    "My program requires the path to the filtered filled in categories database. My program requires the path to the categories and importances table. My program requires the path to the best cards with inventory database. My program requires the path to the condensed cards database. My program relies on the pandas Python library to read and manipulate this databases. My program relies on the random Python class to remove a card among cards with highest converted mana cost to lower average mana cost, and to increase or decrease a number of a certain randomly chosen basic land so to get closer to a total land count of 24. I rely on MTG Arena's basic land calculator to determine actual numbers of basic lands in the strong deck that I play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6-19--Screenshot--Strong_Deck.png\">\n",
    "<br>\n",
    "<center><b>Figure 19:</b> Screenshot of \"Strong Deck.csv\"</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of copies of cards, including lands, in \"Strong_Deck.csv\" is 60.\n",
      "The average converted mana cost among copies of nonland cards in \"Strong_Deck.csv\" is 3.000.\n"
     ]
    }
   ],
   "source": [
    "# Developing_Strong_Deck_List_with_Specific_Mana_Types_Weighted_Categories_and_Max_Ave_CMC.py\n",
    "#\n",
    "# This program creates a strong deck list with mana types specified while filtering the filled-in categories database,\n",
    "# rules-text categories specified when developing the categories-of-interest database\n",
    "# and weighted when developing the categories and importances table,\n",
    "# and a maximum average converted mana cost specified in this program.\n",
    "#\n",
    "# Created: 04/02/19 by Tom Lever\n",
    "# Updated: 04/20/19 by Tom Lever\n",
    "#\n",
    "# Inputs: Filtered_Filled_In_Categories_Database.csv, Categories_and_Importances.csv,\n",
    "# Best_Cards--with_Inventory.csv, Condensed_Cards_Database.csv\n",
    "# Dependences: pandas, numpy\n",
    "# Outputs: Strong_Deck.csv\n",
    "\n",
    "\n",
    "# Allow reading and manipulating dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Allow use of the random.choice method.\n",
    "import random\n",
    "\n",
    "# Allow creation of columns of zeros.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Create a table of names and inventories\n",
    "# for all cards in one or more categories\n",
    "# in the table of categories and importances.\n",
    "#############################################\n",
    "\n",
    "# Read the filtered filled in categories database into a dataframe.\n",
    "filtered_filled_in_categories_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Filtered_Filled_In_Categories_Database.csv\", index_col=0, header=0)\n",
    "\n",
    "# Read the categories and importances table into a dataframe.\n",
    "categories_and_importances = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Categories_and_Importances.csv\", index_col=0)\n",
    "list_of_categories = categories_and_importances.index.tolist()\n",
    "\n",
    "# Filter the filled in categories database into rows\n",
    "# corresponding to cards in the present grouping of categories.\n",
    "mask = filtered_filled_in_categories_database[list_of_categories[0]] == 1\n",
    "for i in range(1, len(list_of_categories)):\n",
    "    mask = mask | filtered_filled_in_categories_database[list_of_categories[i]] == 1\n",
    "cards_in_categories = filtered_filled_in_categories_database[mask]\n",
    "\n",
    "# Create a table of card names and frequencies.\n",
    "names_n_invs = pd.DataFrame(cards_in_categories[\"Inventory\"],\n",
    "                            columns=[\"Inventory\"],\n",
    "                            index=cards_in_categories.index.tolist())\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# Create a table of names, average frequencies, and inventories,\n",
    "# that happens to be sorted first by rarity and second by total number of occurrences.\n",
    "######################################################################################\n",
    "\n",
    "# Enter the best cards with inventory database into a dataframe.\n",
    "best_cards_with_inventory = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Best_Cards--with_Inventory.csv\",\n",
    "                                        header=0,\n",
    "                                        index_col=0)\n",
    "\n",
    "# Create a table of card names and average frequencies from the best cards database.\n",
    "names_n_freqs = pd.DataFrame(best_cards_with_inventory[\"Average Frequency\"].round(decimals=0),\n",
    "                             columns=[\"Average Frequency\"],\n",
    "                             index=best_cards_with_inventory.index.tolist())\n",
    "\n",
    "# Merge the names and inventories dataframe\n",
    "# for cards of appropriate mana types in the present grouping of categories\n",
    "# into the names and average frequencies dataframe\n",
    "# from the best-cards with inventory database,\n",
    "# preserving the order of the best-cards database.\n",
    "names_freqs_n_invs = names_n_freqs.merge(names_n_invs, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Create a table of names, average frequencies, inventories, and desired counts.\n",
    "################################################################################\n",
    "\n",
    "# Create a column for desired counts.\n",
    "desired_counts = []\n",
    "\n",
    "# For each row in the names, frequencies, and inventories dataframe...\n",
    "for i in range(0, names_freqs_n_invs.shape[0]):\n",
    "    \n",
    "    # If the inventory of the card in the present row is greater than the average frequency of the card...\n",
    "    if names_freqs_n_invs.iloc[i][\"Inventory\"] > names_freqs_n_invs.iloc[i][\"Average Frequency\"]:\n",
    "        \n",
    "        # Add the average frequency to the column of desired counts for the present card.\n",
    "        desired_counts.append(names_freqs_n_invs.iloc[i][\"Average Frequency\"])\n",
    "    \n",
    "    # If the inventory of the card in the present row is less than the average frequency of the card...\n",
    "    else:\n",
    "        \n",
    "        # Add the inventory to the column of desired counts for the present card.\n",
    "        desired_counts.append(names_freqs_n_invs.iloc[i][\"Inventory\"])\n",
    "      \n",
    "    # Alternative option:\n",
    "    #desired_counts.append(1)\n",
    "\n",
    "# Create a names, average frequencies, inventories, and desired counts dataframe.\n",
    "names_freqs_invs_n_counts = names_freqs_n_invs\n",
    "names_freqs_invs_n_counts[\"Desired Count\"] = desired_counts\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "# Create a table of names, average frequencies, inventories, desired counts, and converted mana costs.\n",
    "######################################################################################################\n",
    "\n",
    "# Enter the condensed cards daabase into a dataframe.\n",
    "condensed_cards_database = pd.read_csv(\"./Data_With_Ravnica_Allegiance/Condensed_Cards_Database.csv\", index_col=0)\n",
    "\n",
    "# Create a names and converted mana costs dataframe.\n",
    "names_n_CMCs = pd.DataFrame(condensed_cards_database[[\"Card Type\", \"Converted Mana Cost\"]],\n",
    "                            columns=[\"Card Type\", \"Converted Mana Cost\"],\n",
    "                            index=condensed_cards_database.index.tolist())\n",
    "\n",
    "# Merge the names and converted mana costs dataframe\n",
    "# into the names, average frequencies, inventories, and desired counts dataframe,\n",
    "# preserving the order of the best-cards database.\n",
    "names_freqs_invs_counts_n_CMCs = names_freqs_invs_n_counts.merge(names_n_CMCs, left_index=True, right_index=True)\n",
    "names_freqs_invs_counts_n_CMCs[0:5]\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "# Copy the names, average frequencies, inventories, desired counts, and converted mana costs dataframe in memory.\n",
    "# The distinct dataframe in memory will be used as a checklist for reorganizing the cards\n",
    "# corresponding to the chosen mana types and grouping of categories according to the importances of the categories.\n",
    "###################################################################################################################\n",
    "\n",
    "checklist = names_freqs_invs_counts_n_CMCs.copy()\n",
    "\n",
    "# For each card in the checklist...\n",
    "for name in checklist.index.tolist():\n",
    "    \n",
    "    # Drop all lands from the checklist.\n",
    "    # Drop very specific cards from the checklist.\n",
    "    if \"land\" in condensed_cards_database.loc[name][\"Card Type\"].lower():\n",
    "        checklist = checklist.drop(index=name)\n",
    "    if name in [\"Awakened Amalgam\",\n",
    "                \"Desecrated Tomb\",\n",
    "                \"Dragon's Hoard\",\n",
    "                \"Glass of the Guildpact\",\n",
    "                \"Guild Summit\",\n",
    "                \"Sai| Master Thopterist\"]:\n",
    "        checklist = checklist.drop(index=name)\n",
    "\n",
    "# Add an \"Added\" column to keep track of which cards have been added\n",
    "# to a list of card names that will be used as the index for\n",
    "# a strong deck list, the source of strong decks with varying average mana costs.\n",
    "checklist[\"Added\"] = np.zeros(checklist.shape[0])\n",
    "\n",
    "\n",
    "##############################\n",
    "# Create a strong-deck source.\n",
    "##############################\n",
    "\n",
    "# Create storage for a list of card names that will serve as the index for the strong-deck source.\n",
    "list_of_cards = []\n",
    "\n",
    "# Create storage for a column of categories, one category for each card in the strong-deck_source.\n",
    "column_of_cats = []\n",
    "\n",
    "\n",
    "# Create a (number of categories) x (maximum number of cards in a category) dataframe\n",
    "# with all the cards in all of the categories.\n",
    "list_of_cards_in_category_for_each_category = []\n",
    "for category in list_of_categories:\n",
    "    list_of_cards_in_category_for_each_category.append(filtered_filled_in_categories_database[filtered_filled_in_categories_database[category] == 1].index.tolist())\n",
    "categories_and_cards_in_category = pd.DataFrame(list_of_cards_in_category_for_each_category, index=list_of_categories)\n",
    "\n",
    "\n",
    "# While cards remain in the checklist...\n",
    "while checklist.shape[0] > 0:\n",
    "    \n",
    "    # For each category in our present grouping of categories...\n",
    "    for category in list_of_categories:\n",
    "        \n",
    "        # For each slot for a card in the present category in the strong-deck source that should be filled\n",
    "        # before any cards representing any other categories are added to the strong-deck source...\n",
    "        for j in range(0, int(categories_and_importances.loc[category][\"Importance\"])):\n",
    "            \n",
    "            # For each card in the checklist...\n",
    "            for k in range(0, checklist.shape[0]):\n",
    "                \n",
    "                # If fewer copies of the present card have been added to the strong-deck source than are desired, and\n",
    "                # if the present card represents the present category...\n",
    "                if (checklist.iloc[k][\"Added\"] < checklist.iloc[k][\"Desired Count\"]) & (checklist.iloc[k].name in categories_and_cards_in_category.loc[category].tolist()):\n",
    "                    \n",
    "                    # Fill in a slot in index for the strong-deck source with the present card.\n",
    "                    list_of_cards.append(checklist.iloc[k].name)\n",
    "                    \n",
    "                    # Record the present category, which the present card is representing.\n",
    "                    column_of_cats.append(category)\n",
    "                    \n",
    "                    # Note in the checklist that the present card has been added to the strong-deck source.\n",
    "                    row = checklist.iloc[k]\n",
    "                    row.at[\"Added\"] += 1\n",
    "                    checklist.iloc[k] = row\n",
    "                    \n",
    "                    # Move on to the next slot in the strong-deck source.\n",
    "                    break\n",
    "                    \n",
    "    # Remove cards from the checklist that have had all their desired copies added to the strong-deck source.\n",
    "    checklist = checklist[checklist[\"Added\"] != checklist[\"Desired Count\"]].copy()\n",
    "\n",
    "# Create storage for the strong-deck source.\n",
    "strong_deck_list = pd.DataFrame([], columns=names_freqs_invs_counts_n_CMCs.columns, index=[])\n",
    "\n",
    "# Each card in the index for the strong-deck source...\n",
    "for name in list_of_cards:\n",
    "    \n",
    "    # Copy the row from the names, frequencies, desired counts, and converted mana costs dataframe\n",
    "    # corresponding to the present card to the strong-deck source.\n",
    "    row = names_freqs_invs_counts_n_CMCs.loc[name]\n",
    "    row.at[\"Desired Count\"] = 1\n",
    "    strong_deck_list = strong_deck_list.append(row)\n",
    "    \n",
    "# Add the column of categories that each card represents to the strong-deck source.\n",
    "strong_deck_list[\"Category\"] = column_of_cats\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Create a strong deck\n",
    "# with the mana types from the filtered filled-in cards database,\n",
    "# cards representing the categories in the present grouping\n",
    "# in proportions specified in the table of categories and importances, and\n",
    "# a specific average converted mana cost.\n",
    "###########################################################################\n",
    "\n",
    "# Assume the nonland cards in the strong deck\n",
    "# are the 36 best cards in the strong-deck source,\n",
    "# regardless of average converted mana cost.\n",
    "strong_deck = strong_deck_list.iloc[0:36]\n",
    "\n",
    "# While the average converted mana cost among copies of the nonland cards\n",
    "# is greater than some user-defined maximum...\n",
    "while np.mean(strong_deck[\"Converted Mana Cost\"]) > 3:\n",
    "    \n",
    "    # Find the maximum converted mana cost among nonland cards.\n",
    "    max_CMC = strong_deck[\"Converted Mana Cost\"].max()\n",
    "    \n",
    "    # Create a slice of the strong deck with just copies of cards with the maximum CMC.\n",
    "    most_costly_cards__rows = strong_deck[strong_deck[\"Converted Mana Cost\"] == max_CMC]\n",
    "    \n",
    "    # Drop duplicates from the slice of the strong deck with copies of cards with the max CMC.\n",
    "    most_costly_cards__rows__dups_dropped = most_costly_cards__rows.drop_duplicates()\n",
    "    \n",
    "    # Create a list of the names of cards in the condensed slice.\n",
    "    most_costly_cards__names__dups_dropped = most_costly_cards__rows__dups_dropped.index.tolist()\n",
    "    \n",
    "    # Select a card from the list of cards in the condensed slice.\n",
    "    card_to_replace__name = random.choice(most_costly_cards__names__dups_dropped)\n",
    "\n",
    "    # Find the row in the condensed slice corresponding to the selected card.\n",
    "    card_to_replace__row = most_costly_cards__rows__dups_dropped.loc[card_to_replace__name]\n",
    "    \n",
    "    # Record the converted mana cost of the selected card,\n",
    "    # which is the maximum converted mana cost.\n",
    "    card_to_replace__CMC = card_to_replace__row[\"Converted Mana Cost\"]\n",
    "    \n",
    "    # Find the category of the selected card.\n",
    "    card_to_replace__category = card_to_replace__row[\"Category\"]\n",
    "    \n",
    "    # For each row corresponding to a card in the strong deck...\n",
    "    for i in range(0, strong_deck.shape[0]):\n",
    "        \n",
    "        # If the present row is matches the card to replace...\n",
    "        if strong_deck.iloc[i].equals(card_to_replace__row):\n",
    "            \n",
    "            # Record the position of the row to replace in the strong deck.\n",
    "            pos_of_card_to_replace_in_strong_deck = i\n",
    "            \n",
    "            # Stop searching the strong deck for the row to replace.\n",
    "            break\n",
    "    \n",
    "    # Send the index of the strong deck to a list.\n",
    "    strong_deck__names = strong_deck.index.tolist()\n",
    "    \n",
    "    # Remove the name of the selected card to remove from the index for the strong deck.\n",
    "    del strong_deck__names[pos_of_card_to_replace_in_strong_deck]\n",
    "    \n",
    "    # Send the categories that cards in the strong deck represent to a list. \n",
    "    strong_deck__categories = strong_deck[\"Category\"].tolist()\n",
    "    \n",
    "    # Remove the category of the selected c ard to remove from the column of categories.\n",
    "    del strong_deck__categories[pos_of_card_to_replace_in_strong_deck]\n",
    "    \n",
    "    # Recreate the strong deck without the removed card.\n",
    "    strong_deck = pd.DataFrame([], columns=names_freqs_invs_counts_n_CMCs.columns, index=[])\n",
    "    for name in strong_deck__names:\n",
    "        row = names_freqs_invs_counts_n_CMCs.loc[name]\n",
    "        row.at[\"Desired Count\"] = 1\n",
    "        strong_deck = strong_deck.append(row)\n",
    "    strong_deck[\"Category\"] = strong_deck__categories\n",
    "\n",
    "    # Keep track of whether a replacement card with a lesser mana cost than that of the removed card\n",
    "    # has been added to the strong deck.\n",
    "    was_card_added = False\n",
    "    \n",
    "    # For each card back in the strong-deck list...\n",
    "    for i in range(0, strong_deck_list.shape[0]):\n",
    "        \n",
    "        # If the converted mana cost of the present card is less than the CMC of the card that was replaced...\n",
    "        if strong_deck_list.iloc[i][\"Converted Mana Cost\"] < card_to_replace__CMC:\n",
    "            \n",
    "            # If the present card represents the category that the removed card represented...\n",
    "            if strong_deck_list.iloc[i][\"Category\"] == card_to_replace__category:\n",
    "                \n",
    "                # If the number of copies of the present card in the new strong deck\n",
    "                # is less than the desired count for copies of the present card...\n",
    "                if strong_deck[strong_deck.index == strong_deck_list.iloc[i].name].shape[0] < strong_deck_list.iloc[i][\"Desired Count\"]:\n",
    "                    \n",
    "                    # Add the row corresponding to the replacement card from the strong-deck source\n",
    "                    # to the strong deck.\n",
    "                    strong_deck = strong_deck.append(strong_deck_list.iloc[i])\n",
    "                    \n",
    "                    # Note that a card was added to the strong deck to replace the removed card.\n",
    "                    was_card_added = True\n",
    "                    \n",
    "                    # Stop looking for a replacement card.\n",
    "                    break\n",
    "                    \n",
    "    # If no card was found in the strong-deck list that met all the criteria above...\n",
    "    if not was_card_added:\n",
    "        \n",
    "        # For each card in the strong-deck source...\n",
    "        for i in range(0, strong_deck_list.shape[0]):\n",
    "            \n",
    "            # If the CMC of the present card is less than the CMC of the card that was replaced...\n",
    "            if strong_deck_list.iloc[i][\"Converted Mana Cost\"] < card_to_replace__CMC:\n",
    "                \n",
    "                # If the present card in the strong-deck source represents the \"flying\" category...\n",
    "                if strong_deck_list.iloc[i][\"Category\"] == \"flying\":\n",
    "                    \n",
    "                    # If the number of copies of the present card in the new strong deck\n",
    "                    # is less than the desired count for copies of the present card...\n",
    "                    if strong_deck[strong_deck.index == strong_deck_list.iloc[i].name].shape[0] < strong_deck_list.iloc[i][\"Desired Count\"]:\n",
    "                        \n",
    "                        # Add the row corresponding to the replacement card from the strong-deck source\n",
    "                        # to the strong deck.\n",
    "                        strong_deck = strong_deck.append(strong_deck_list.iloc[i])\n",
    "                        \n",
    "                        # Note that a card was added to the strong deck to replace the removed card.                        \n",
    "                        was_card_added = True\n",
    "                        \n",
    "                        # Stop looking for a replacement card.                        \n",
    "                        break\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Add lands at the end of the strong deck.\n",
    "##########################################\n",
    "\n",
    "# Create a table of names and inventories for all cards in the filtered filled-in categories database.\n",
    "names_n_invs = pd.DataFrame(filtered_filled_in_categories_database[\"Inventory\"].tolist(),\n",
    "                              columns=[\"Inventory\"],\n",
    "                              index=filtered_filled_in_categories_database.index.tolist())\n",
    "\n",
    "# Create a dataframe of names, rarities, total number of occurrences, and average frequencies\n",
    "# for all cards in the best cards with inventory database.\n",
    "names_rarities_occurs_n_freqs = pd.DataFrame([],\n",
    "                                             columns=[\"Rarity\", \"Total Number of Occurrences\", \"Average Frequency\"],\n",
    "                                             index=best_cards_with_inventory.index.tolist())\n",
    "names_rarities_occurs_n_freqs[\"Rarity\"] = best_cards_with_inventory[\"Rarity\"]\n",
    "names_rarities_occurs_n_freqs[\"Total Number of Occurrences\"] = best_cards_with_inventory[\"Total Number of Occurrences\"]\n",
    "names_rarities_occurs_n_freqs[\"Average Frequency\"] = best_cards_with_inventory[\"Average Frequency\"]\n",
    "\n",
    "# Merge the names and inventories table into the names, rarities, occurrences, and frequencies dataframe,\n",
    "# preserving the order of the best-cards database.\n",
    "names_rarities_occurs_freqs_n_invs = names_rarities_occurs_n_freqs.merge(names_n_invs, left_index=True, right_index=True)\n",
    "\n",
    "# Create a names and card types table from the condensed cards database.\n",
    "names_n_card_types = pd.DataFrame(condensed_cards_database[\"Card Type\"].tolist(),\n",
    "                                  columns=[\"Card Type\"],\n",
    "                                  index=condensed_cards_database.index.tolist())\n",
    "\n",
    "# Create a dataframe of information on lands with appropriate mana types.\n",
    "filtered_best_lands = names_rarities_occurs_freqs_n_invs.merge(names_n_card_types,\n",
    "                                                               left_index=True,\n",
    "                                                               right_index=True)\n",
    "filtered_best_lands = filtered_best_lands[filtered_best_lands[\"Card Type\"].str.lower().str.contains(\"land\")]\n",
    "filtered_best_lands = filtered_best_lands.round(0)\n",
    "\n",
    "# Drop lands that I don't like from the dataframe of lands.\n",
    "for name in filtered_best_lands.index.tolist():\n",
    "    if name in [\"Plaza of Harmony\", \"Reliquary Tower\", \"Rupture Spire\", \"Gateway Plaza\", \"Unknown Shores\"]:\n",
    "        filtered_best_lands = filtered_best_lands.drop(index=name).round(0)\n",
    "\n",
    "# Create a list of desired counts for the lands based on average frequencies and inventories.\n",
    "desired_counts = []\n",
    "for i in range(0, filtered_best_lands.shape[0]):\n",
    "    if filtered_best_lands.iloc[i][\"Inventory\"] > filtered_best_lands.iloc[i][\"Average Frequency\"]:\n",
    "        desired_counts.append(filtered_best_lands.iloc[i][\"Average Frequency\"])\n",
    "    else:\n",
    "        desired_counts.append(filtered_best_lands.iloc[i][\"Inventory\"])\n",
    "        \n",
    "# Add the desired count list as a column to the dataframe of information on lands.\n",
    "filtered_best_lands[\"Desired Count\"] = desired_counts\n",
    "\n",
    "# Add a column of zeros indicating converted mana costs of zero for lands to the dataframe of information on lands.\n",
    "filtered_best_lands[\"Converted Mana Cost\"] = np.zeros(filtered_best_lands.shape[0])\n",
    "\n",
    "# Add a column indicating that lands fall into their own category of \"Land\".\n",
    "filtered_best_lands[\"Category\"] = filtered_best_lands[\"Card Type\"]\n",
    "filtered_best_lands = filtered_best_lands[[\"Average Frequency\", \"Inventory\", \"Desired Count\", \"Card Type\", \"Converted Mana Cost\", \"Category\"]]\n",
    "\n",
    "# Play with the numbers of basic lands to get the total number of lands to 24.\n",
    "while filtered_best_lands[\"Desired Count\"].sum() < 24:\n",
    "    basic_land_to_increase = random.choice([\"Forest\", \"Island\", \"Mountain\", \"Plains\", \"Swamp\"])\n",
    "    if basic_land_to_increase in filtered_best_lands.index.tolist():\n",
    "        filtered_best_lands.at[basic_land_to_increase, \"Desired Count\"] += 1\n",
    "while filtered_best_lands[\"Desired Count\"].sum() > 24:\n",
    "    basic_land_to_increase = random.choice([\"Forest\", \"Island\", \"Mountain\", \"Plains\", \"Swamp\"])\n",
    "    if basic_land_to_increase in filtered_best_lands.index.tolist():\n",
    "        filtered_best_lands.at[basic_land_to_increase, \"Desired Count\"] -= 1\n",
    "\n",
    "# Add 24 appropriate lands with category \"Land\" to the strong deck.\n",
    "strong_deck = strong_deck.append(filtered_best_lands)\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "# Output the strong deck to a CSV file.\n",
    "# Output the total number of copies of cards in the deck to this notebook.\n",
    "# Output the average converted mana cost among nonland cards in the strong deck to this notebook.\n",
    "#################################################################################################\n",
    "\n",
    "# Write the strong deck to a CSV file.\n",
    "strong_deck.to_csv(\"./Data_With_Ravnica_Allegiance/Strong_Deck.csv\")\n",
    "\n",
    "print(\"The total number of copies of cards, including lands, in \\\"Strong_Deck.csv\\\" is \" + \"%.f\" % strong_deck[\"Desired Count\"].sum() + \".\")\n",
    "print(\"The average converted mana cost among copies of nonland cards in \\\"Strong_Deck.csv\\\" is \" + \"%.3f\" % np.mean(strong_deck[\"Converted Mana Cost\"][strong_deck[\"Converted Mana Cost\"] > 0]) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. An FP deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"Ranked\" mode in MTG Arena, at Gold Tier 3, a (C / F / P / CF / CP / FP / CFP), a (\"create creature token or convert lands into creatures\", \"destroy\", \"draw\", \"put nonland card\", \"flying\") deck, with an unconstrained maximum average mana cost among nonland cards and an actual cost of 3.444 (which felt a little heavy):\n",
    "<ol>\n",
    "    <li>Lost against an unblockable / power up / draw / counter / hexproof / boost toughness; Mist-Cloaked Herald / Curious Obsession / Pteramander; I deck.\n",
    "    <li>Won against a menace / deathtouch / first strike; Immolation Shaman / Goblin Chainwhirler; MS deck.\n",
    "    <li>Won against an Orzhov Enforcer; PS deck.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major limitations associated with the win / loss ratios for the many decks that ultimately impact the accuracy of the win / loss ratios associated with groupings of categories. First, while I obviously ignored decks without win / loss ratios, I did not ignore decks with win / loss ratios of 0 or 100. Given my play results and experience, it seems unlikely that so many decks would have long-term average win / loss ratios of 0 or 100. I think eliminating these decks too would increase the accuracy and actually provide more spread to the win / loss ratios associated with groupings of categories. However, I preferred keeping them to increase the number of cards in the best-cards with inventory database and to spread the total numbers of occurrences of these cards.\n",
    "\n",
    "Closely related to the above limitation, I was not able to easily scrape together information on how many game results were included in the total win and total loss numbers for each deck. There is an implicit assumption that each deck was played the same number of times and an infinite number of times. I believe such total win and total loss number information can be found by interpreting graphs on each of MTGArena.pro’s deck pages. However, it did seem that many of the decks were played for similar amounts of time, which might suggest that they were played a similar number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By writing and using software to answer my research question of, basically, “What is a strong-deck list meeting the five specified criteria in the research question section?”, I assembled a strong-deck list that turned out to win two out of the three games that I played with it. I would say that this method of finding strong deck lists is a “win”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Acknowledgements and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am grateful to:\n",
    "\n",
    "<ol>\n",
    "    <li>EdX.org and the University of California: San Diego for offering a free \"Python for Data Science\" course geared toward graduate students, which got me started with Python, database manipulation, machine-learning, and natural-language processing.\n",
    "    <li>Wizards of the Coast for offering a web-based MTG cards database and a gameplay environment that are easy and enjoyable to use.\n",
    "    <li>MTGArena.pro for offering an extensive community-supported decks database.\n",
    "    <li>All the contributors to StackOverflow.com for providing solutions and workarounds to conception and implementation difficulties I ran into in writing the Jupyter Notebook.\n",
    "    <li>The community-oriented (I assume) authors of all the Python modules (including web scraping, database manipulation, and machine-learning modules) that were essential to my work.\n",
    "    <li>The authors of the approachable and practical \"Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit\", which provides me food for thought in how to analyze my rules text database more effectively and begin to study natural language processing and artificial intelligence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
